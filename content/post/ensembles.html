---
title: 'BAIT 509 Class Meeting 07'
subtitle: "Ensemble methods"
date: "January 21, 2019"
output: 
    html_document:
        keep_md: true
        toc: true
        toc_depth: 2
        number_sections: true
        theme: cerulean
        toc_float: true
---



<div id="motivation-why-ensembles" class="section level1">
<h1>Motivation: why ensembles?</h1>
<p>Because a classification or regression tree alone tends to be a poor competitor against other machine learning methods. In particular, they tend to be sensitive to the data: if fit to a separate training set, a completely different tree is prone to being fit. This is an embodiment of <em>high variance</em>.</p>
<p>Consider the hypothetical situation: collect <span class="math inline">\(B\)</span> data sets (of equal size), and fit a tree to all <span class="math inline">\(B\)</span> of them. These <span class="math inline">\(B\)</span> models are called an <strong>ensemble</strong>. Then if we want to predict on a new case (i.e., we’ve observed the predictors but not the response), take the average predictions of the <span class="math inline">\(B\)</span> trees in the case of regression, or the popular vote of the <span class="math inline">\(B\)</span> trees in the case of classification. This will reduce variance.</p>
<p>But collecting <span class="math inline">\(B\)</span> data sets is not practical, and splitting our single data set into <span class="math inline">\(B\)</span> parts would not be a substitute, because we’d only be skyrocketing the variance of each tree before then reducing it in the ensemble.</p>
</div>
<div id="bagging-bootstrap-aggregating" class="section level1">
<h1>Bagging (= Bootstrap Aggregating)</h1>
<p>We can emulate the above hypothetical situation with a technique called the <strong>bootstrap</strong>. If your data set has <span class="math inline">\(n\)</span> observations, then we can draw (for all intents and purposes) any number <span class="math inline">\(B\)</span> of data sets we like. To generate one data set, just randomly sample <span class="math inline">\(n\)</span> observations <strong>with replacement</strong>, and ta-da!</p>
<p>The <span class="math inline">\(B\)</span> data sets are related in some sense, so are not as good as having <span class="math inline">\(B\)</span> <em>independent</em> data sets. But it still gives us something useful – in fact, tremendously useful. Go ahead and fit a tree on each data set, and combine the results – that’s bagging in a nutshell.</p>
<p>Note that we deliberately tend to <strong>overfit</strong> each tree in the ensemble, to get trees with low bias and high variance – the variance of which will be reduced in the ensemble.</p>
<p>This concept of bootstrap is very widespread – it’s not just used for trees, and not even just for machine learning. But for BAIT 509, trees are the only context you’ll see bootstrap in.</p>
<div id="size-of-b" class="section level2">
<h2>Size of <span class="math inline">\(B\)</span></h2>
<p>Note that we can’t overfit by increasing <span class="math inline">\(B\)</span>, because this just results in new data sets being generated – not fitting more and more models to a single data set. The error (MSE or error rate) will drop as <span class="math inline">\(B\)</span> increases, until it reaches a stable point where it no longer drops. Once this point is reached, increasing <span class="math inline">\(B\)</span> does not do us much good. This is a common approach for choosing <span class="math inline">\(B\)</span> – no need for cross validation.</p>
</div>
<div id="out-of-bag-oob-error" class="section level2">
<h2>Out of Bag (OOB) error</h2>
<p>With bagging comes a unique opportunity to calculate the out-of-sample/test error <em>without having to partition the data</em> into training and test sets (either through the validation set method, or cross validation). Here’s how.</p>
<p>Remember how we obtain a single bootstrap data set: sample with replacement <span class="math inline">\(n\)</span> times. This means that, for each bootstrap sample, there will be some observations that were left out. These are called <strong>out-of-bag</strong> (OOB) observations, and naturally form a test set!</p>
<p>Now, obtain an estimate of generalization error (such as MSE or error rate) like so: for each observation in your full data set, consider the trees for which this observation is OOB, and use those to form an ensemble prediction. Compare this against the true value to get the error for this observation. Repeat for all observations.</p>
</div>
<div id="predictor-importance" class="section level2">
<h2>Predictor Importance</h2>
<p>We can use ensembles to determine the <em>importance</em> of certain predictors over others. Recall that the addition of a stump to a tree reduces the training error. We can set up a “points system”, where a reduction in MSE is “awarded” to the predictor responsible for the stump. Do this for all nodes in a tree to come up with a final score for each predictor – the ones with the largest scores are most important.</p>
</div>
</div>
<div id="random-forests" class="section level1">
<h1>Random Forests</h1>
<p>One problem with Bagging is that the trees in the ensemble tend to be <em>correlated</em> – that is, they share similarities.</p>
<p>Random forests attempt to fix this problem by modifying how a single tree in the ensemble is grown. Recall that, when making a new stump to grow a tree, we choose one predictor out of the total <span class="math inline">\(p\)</span> predictors. The idea behind random forests is to <em>restrict this choice to some random subset</em> of <span class="math inline">\(m\)</span> predictors out of the <span class="math inline">\(p\)</span>. A new batch of <span class="math inline">\(m\)</span> predictors is selected each time a stump is to be made.</p>
<p>The result is an ensemble of trees that look “more random” – they are said to be <em>decorrelated</em>. This prevents any one predictor from “dominating” the ensemble. And because the trees are less related, combining their predictions results in an overall better result.</p>
</div>
<div id="discussion-questions" class="section level1">
<h1>Discussion Questions</h1>
<ul>
<li>Bagging is a special case of random forests under which case?</li>
<li>What are the hyperparameters we can control for random forests?</li>
<li>Suppose you have the following paired data of <code>(x,y)</code>: (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
<ol style="list-style-type: decimal">
<li>(1,0), (1,2), (1,5)</li>
<li>(1,2), (2,0)</li>
<li>(1,2), (1,2), (1,5)</li>
</ol></li>
<li>For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?</li>
<li>You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
<ol style="list-style-type: decimal">
<li>Regression: your trees make the following four predictions: 1,1,3,3.</li>
<li>Classification: your trees make the following four predictions: “A”, “A”, “B”, “C”.</li>
</ol></li>
</ul>
</div>
<div id="boosting" class="section level1">
<h1>Boosting</h1>
<p>Boosting is another method, different from random forests and bagging, but still involves combining predictions of an ensemble.</p>
<p>The details are beyond the scope of this course, so we will explain the main ideas. If you truly want a more comprehensive treatment, I suggest reading <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">this Kaggle blog post</a>.</p>
<div id="basic-boosting" class="section level2">
<h2>Basic boosting</h2>
<p>(Also see the “Motivation” part of the above Kaggle blog – this part is not overly technical).</p>
<p>Let’s look at a simple two-tree boosting ensemble for regression.</p>
<ol style="list-style-type: decimal">
<li>Fit a tree to your data</li>
<li>Compute the residuals (actual minus prediction).</li>
<li>Fit a second tree <em>to the residuals</em>.</li>
</ol>
<p>To make a prediction on a new observation, do the following:</p>
<ol style="list-style-type: decimal">
<li>Feed the predictors into the first tree to get a “preliminary” prediction.</li>
<li>Feed the predictors into the second tree to get an <em>adjustment</em>.</li>
<li>Obtain a final prediction by adding the adjustment to the preliminary prediction.</li>
</ol>
<p>This second tree captures patterns in the data that the first tree missed, which is why boosting is so useful.</p>
<p>Boosting, then, is a continuation of this, fitting trees in sequence.</p>
</div>
<div id="a-conglomerate-of-weak-learners" class="section level2">
<h2>A conglomerate of “weak learners”</h2>
<p>Boosting gradually improves predictions by learning on residuals. Because of this, there is no need to build a “strong” model for each iteration – i.e., one that does well at prediction.</p>
<p>Instead, we deliberately build <em>weak models</em> to slowly get at the structure underyling the data. We therefore build low-depth trees for each member of the ensemble.</p>
</div>
<div id="boosting-for-classification" class="section level2">
<h2>Boosting for Classification</h2>
<p>The adjustments made here are less interpretable, but do follow a similar logic. Instead of learning on residuals, a consecutive tree leans on classes <em>re-weighted</em> so that observations that are incorrectly classified get a higher weight. This is called <strong>adaboost</strong>.</p>
</div>
<div id="learning-rate" class="section level2">
<h2>Learning Rate</h2>
<p>It turns out that we obtain a more powerful prediction if we <em>slow down</em> the “rate of learning”. We introduce a “rate of learning” hyperparameter <span class="math inline">\(\lambda\)</span> between 0 and 1. Predictions from trees are multiplied by this amount before adjusting the prediction from the previous tree.</p>
</div>
</div>
<div id="ensembles-in-r" class="section level1">
<h1>Ensembles in R</h1>
<div id="randomforest" class="section level2">
<h2><code>randomForest</code></h2>
<p>We use the <code>randomForest</code> package and the <code>randomForest</code> function in R to implement random forests (and thus also bagging for classification and regression trees). The syntax follows R’s regression paradigm with <code>randomForest(response~predictors, data)</code>. Let’s see an example with the <code>mtcars</code> data (a default data frame in R), predicting <code>mpg</code> (a regression problem):</p>
<pre class="r"><code>suppressPackageStartupMessages(library(randomForest))
set.seed(40)
my_fit &lt;- randomForest(mpg ~ ., data=mtcars)</code></pre>
<p>Note that the <code>.</code> stands for “all other variables in the data frame”.</p>
<p>The <code>plot</code> function is useful for giving a quick-and-dirty plot of error vs. <span class="math inline">\(B\)</span>:</p>
<pre class="r"><code>plot(my_fit)</code></pre>
<p><img src="/tutorials/ensembles_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>There’s stability after around <span class="math inline">\(B=200\)</span> trees.</p>
<p><strong>Warning!</strong> The <code>predict</code> function works differently than usual. Usually, the following two outputs would be the same:</p>
<pre class="r"><code>yhat1 &lt;- predict(my_fit)
yhat2 &lt;- predict(my_fit, newdata=mtcars)</code></pre>
<p>But they’re not. Take a look:</p>
<pre class="r"><code>plot(yhat1, yhat2)</code></pre>
<p><img src="/tutorials/ensembles_files/figure-html/unnamed-chunk-4-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>What’s going on here? It turns out <code>predict(my_fit)</code> <em>without</em> specifying <code>newdata</code> gives the <em>out of bag predictions</em>, whereas the entire ensemble is used to make predictions when the <code>newdata</code> argument is specified. So the OOB error and training errors, respectively, are:</p>
<pre class="r"><code>mean((yhat1 - mtcars$mpg)^2)</code></pre>
<pre><code>## [1] 5.655269</code></pre>
<pre class="r"><code>mean((yhat2 - mtcars$mpg)^2)</code></pre>
<pre><code>## [1] 1.49721</code></pre>
<p>Here are two key parameters you can change in the <code>randomForest</code> function:</p>
<ul>
<li><code>mtry</code>: The number of predictors to sample at every stump iteration.
<ul>
<li>Set equal to the total number of predictors to perform bagging.</li>
</ul></li>
<li><code>ntree</code>: The number of trees to fit.</li>
</ul>
</div>
<div id="boosting-1" class="section level2">
<h2>Boosting</h2>
<p>Boosting is not on your Assignment 2. For your reference, I’ll indicate some useful implementations here.</p>
<ul>
<li>For regression, you can use the <code>gbm</code> function from the <code>gbm</code> R package to do boosting. But you can’t do classification beyond 2 classes.</li>
<li>For classification, I recommend the <code>AdaBoostClassifier</code> from the <code>sklearn.ensemble</code> library.
<ul>
<li>This library also contains <code>RandomForestClassifier</code> for random forest classification.</li>
<li>For plain decision trees (again for classification), the <code>sklearn.tree</code> library has a <code>DecisionTreeClassifier</code> method.</li>
</ul></li>
</ul>
</div>
</div>
