<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Vincenzo Coia</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 18 Feb 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>The missing question in supervised learning</title>
      <link>/post/2018-02-18-mean/</link>
      <pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-02-18-mean/</guid>
      <description>&lt;p&gt;You all know the drill &amp;ndash; you&amp;rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34;&gt;Regression trees&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Local_regression&#34;&gt;loess&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression&#34;&gt;linear regression&lt;/a&gt;&amp;hellip; you name it, they&amp;rsquo;re all in pursuit of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34;&gt;mean&lt;/a&gt; (well, almost all). But the true outcome is random. It has a distribution. Are you sure you want the mean of that distribution?&lt;/p&gt;
&lt;p&gt;You might say &amp;ldquo;Yes! It ensures my prediction is as close as possible to the outcome!&amp;rdquo; If this is indeed what you want, the mean still might not be your best choice &amp;ndash; it only ensures the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34;&gt;mean squared error&lt;/a&gt; is minimized.&lt;/p&gt;
&lt;p&gt;There are a suite of other options that might be more appropriate than the mean. The good thing is, your favourite supervised learning method probably has a natural extension for estimating these alternatives. Let&amp;rsquo;s investigate the quantities you might care about.&lt;/p&gt;
&lt;h3 id=&#34;the-median&#34;&gt;The Median&lt;/h3&gt;
&lt;p&gt;No, the median &lt;em&gt;isn&amp;rsquo;t&lt;/em&gt; just an inferior version of the mean, to be used under the unfortunate presence of outliers.&lt;/p&gt;
&lt;p&gt;If I randomly pick a data scientist, what do you think their salary would be? This distribution has a right-skew, so chances are, your data scientist earns less than the mean. Predict the &lt;a href=&#34;https://en.wikipedia.org/wiki/Median&#34;&gt;median&lt;/a&gt;, and you&amp;rsquo;ll have a 50% chance that your data scientist &lt;em&gt;does&lt;/em&gt; earn at least what you predict.&lt;/p&gt;
&lt;p&gt;In short, use the median when you want your prediction to be exceeded with a coin toss.&lt;/p&gt;
&lt;p&gt;Minimize the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_absolute_error&#34;&gt;mean absolute error&lt;/a&gt; to get this prediction.&lt;/p&gt;
&lt;h3 id=&#34;higher-or-lower-quantiles&#34;&gt;Higher (or lower) Quantiles&lt;/h3&gt;
&lt;p&gt;Want to make it to an interview on time? You add some &amp;ldquo;buffer time&amp;rdquo; to the expected travel time, right? What you&amp;rsquo;re after is a high &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile&#34;&gt;quantile&lt;/a&gt; of travel time &amp;ndash; something like the 0.99-quantile, so that there is only a small chance you&amp;rsquo;ll be late (1% in this case).&lt;/p&gt;
&lt;p&gt;Use a high (or low) quantile if you want a conservative (or liberal) prediction &amp;ndash; or both, if you want a prediction interval.&lt;/p&gt;
&lt;p&gt;Minimize the mean &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantile_regression#Quantiles&#34;&gt;rho function&lt;/a&gt; to get this prediction.&lt;/p&gt;
&lt;h3 id=&#34;the-mean&#34;&gt;The Mean&lt;/h3&gt;
&lt;p&gt;The mean is useful when we care about &lt;em&gt;totals&lt;/em&gt;. Want to know how much gas a vehicle uses?  You&amp;rsquo;re after the mean, because the total quantity drawn out over time is what matters.&lt;/p&gt;
&lt;p&gt;Minimize the mean squared error to get this prediction.&lt;/p&gt;
&lt;h3 id=&#34;other-options&#34;&gt;Other Options&lt;/h3&gt;
&lt;p&gt;Do you really need to distill your prediction down to a single number? Consider looking at the entire distribution of the outcome as your prediction (typically conditional on predictors) &amp;ndash; after all, this conveys the entire uncertainty about the outcome. This is known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Probabilistic_forecasting&#34;&gt;probabilistic forecasting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are other measures, too. &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_shortfall&#34;&gt;Expected shortfall&lt;/a&gt; is useful for risk analysis, or even &lt;a href=&#34;http://www.statcan.gc.ca/pub/12-001-x/2016001/article/14545/03-eng.htm&#34;&gt;expectiles&lt;/a&gt;. Maybe you care about variance or skewness for some reason. Whatever you want to get at, just make sure you ask yourself what you actually care about. You have an entire distribution to distill!&lt;/p&gt;
&lt;p&gt;(&lt;a href=&#34;https://www.pexels.com/photo/abstract-blackboard-bulb-chalk-355948/&#34;&gt;Photo from Pexels&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contour Plots</title>
      <link>/post/contour_plots/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/contour_plots/</guid>
      <description>


&lt;p&gt;This tutorial introduces contour plots, and how to plot them in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;what-is-a-contour-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a contour plot?&lt;/h1&gt;
&lt;p&gt;Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?&lt;/p&gt;
&lt;p&gt;The idea is to use &lt;em&gt;contour lines&lt;/em&gt;, which are curves that indicate a constant height.&lt;/p&gt;
&lt;p&gt;Imagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude. Then trace (on your map) the shapes formed by the new (flat) mountain tops. These curves are contour lines. Choose a differential such as 50 meters, and draw these curves for altitudes …800m, 850m, 900m, 950m, 1000m, … – the result is a &lt;strong&gt;contour plot&lt;/strong&gt; (or topographic map, if it’s a map).&lt;/p&gt;
&lt;p&gt;In general, contour plots are useful for functions of two variables (like a bivariate gaussian density).&lt;/p&gt;
&lt;p&gt;We’ll look at examples in the next section.&lt;/p&gt;
&lt;p&gt;Notes on contours:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They never cross.&lt;/li&gt;
&lt;li&gt;The steepest slope at a point is parallel to the contour line.&lt;/li&gt;
&lt;li&gt;They aren’t entirely ambiguous. For example, you can’t tell whether or not the mountains are actually mountains, or whether they’re holes/valleys! Sometimes you can add colour to indicate depth; other times (like in topographic maps) you can indicate elevation directly as numbers beside contour lines. Other times, this is not required, because the context makes it obvious.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;contour-plots-in-ggplot2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Contour plots in &lt;code&gt;ggplot2&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;There are two ways you can make contour plots in &lt;code&gt;ggplot2&lt;/code&gt; – but they’re both for quite different purposes.&lt;/p&gt;
&lt;div id=&#34;method-1-approximate-a-bivariate-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method 1: Approximate a bivariate density&lt;/h2&gt;
&lt;p&gt;This method approximates a bivariate density &lt;strong&gt;from data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, recall how this is done in the univariate case. A little kernel function (like a shrunken bell curve) is placed over each data point, and these are added together to get a density estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
set.seed(373)
x &amp;lt;- rnorm(1000)
ggplot(data.frame(x=x), aes(x)) + 
    geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do the same thing to get a bivariate density, except with little bivariate kernel functions (like shrunken bivariate Gaussian densities). But, we can’t just simply put “density height” on the vertical axis – we need that for the second dimension. Instead, we can use contour plots.&lt;/p&gt;
&lt;p&gt;This is the contour plot that &lt;code&gt;ggplot2&lt;/code&gt;’s &lt;code&gt;geom_density2d()&lt;/code&gt; does: builds a bivariate kernel density estimate (based on data), then makes a contour plot out of it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- rnorm(1000)
ggplot(data.frame(x=x, y=y), aes(x, y)) + 
    geom_density2d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on context (this is a density), we know that this is a “hill” and not a “hole”. If you were to start at some point at the “bottom” of the hill, the steepest way up would be perpendicular to the contours. The highest point on the hill is within the middle-most circle.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-2-general-contour-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method 2: General Contour Plots&lt;/h2&gt;
&lt;p&gt;You can also make contour plots that &lt;em&gt;aren’t&lt;/em&gt; a kernel density estimate (necessarily), using &lt;code&gt;geom_contour()&lt;/code&gt;. This is based off of &lt;strong&gt;any bivariate function&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basics&lt;/h3&gt;
&lt;p&gt;Suppose we want to make a contour plot of the bivariate function &lt;span class=&#34;math display&#34;&gt;\[f(x,y) = x^2 + sin(y)\]&lt;/span&gt; over the rectangle &lt;span class=&#34;math inline&#34;&gt;\(-2&amp;lt;x&amp;lt;2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-5&amp;lt;y&amp;lt;5\)&lt;/span&gt;. First, make a grid over the rectangle (it &lt;em&gt;must&lt;/em&gt; be a grid – &lt;code&gt;geom_contour()&lt;/code&gt; won’t work otherwise). Then, evaluate the function at each of the grid points. Put all this info into a single data frame with three columns (two for the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; coordinates, and one for the function evaluation). Then, indicate the function evaluation in &lt;code&gt;geom_contour()&lt;/code&gt; as the aesthetic &lt;code&gt;z&lt;/code&gt;, and the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; aesthetics are as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- function(x) x[1]^2 + sin(x[2])
x &amp;lt;- seq(-2, 2, length.out=100)
y &amp;lt;- seq(-5, 5, length.out=100)
dat &amp;lt;- expand.grid(x=x, y=y)  # Data frame of 100*100=10000 points.
dat$z &amp;lt;- apply(dat, 1, f)
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;expand.grid&lt;/code&gt; is useful for making grids. It returns all pairs from the input vectors. But, this also means that it’s easy for the output to explode!&lt;/p&gt;
&lt;p&gt;Note that finer grids yield plots with higher accuracy. Here’s an example of a rough grid, whose contours are jagged:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- function(x) x[1]^2 + sin(x[2])
x &amp;lt;- seq(-2, 2, length.out=10)
y &amp;lt;- seq(-5, 5, length.out=10)
dat &amp;lt;- expand.grid(x=x, y=y) # Data frame of 10*10=100 points.
dat$z &amp;lt;- apply(dat, 1, f)
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-settings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additional Settings&lt;/h3&gt;
&lt;p&gt;Here, we’ll look at colouring the plots, and adding more/less contours.&lt;/p&gt;
&lt;p&gt;Here’s another example, with the &lt;code&gt;volcano&lt;/code&gt; data (a matrix of altitudes for a volcano). If you’d like, first take a look at a 3D rendering of the volcano, by running the following code chunk in your R console after un-commenting the last two lines (code taken directly from &lt;code&gt;rgl&lt;/code&gt;’s &lt;code&gt;surface3d()&lt;/code&gt; documentation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(volcano)
z &amp;lt;- 2 * volcano        # Exaggerate the relief
x &amp;lt;- 10 * (1:nrow(z))   # 10 meter spacing (S to N)
y &amp;lt;- 10 * (1:ncol(z))   # 10 meter spacing (E to W)
zlim &amp;lt;- range(y)
zlen &amp;lt;- zlim[2] - zlim[1] + 1
colorlut &amp;lt;- terrain.colors(zlen) # height color lookup table
col &amp;lt;- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point
# open3d()
# surface3d(x, y, z, color = col, back = &amp;quot;lines&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feel free to move the image around by clicking and dragging. Neat, eh?&lt;/p&gt;
&lt;p&gt;We’ll make a contour plot with this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(x=x, y=y)
dat$z &amp;lt;- as.vector(z)/2   # &amp;quot;De-exaggerate&amp;quot; the relief
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z)) +
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But, you can’t tell that the inner circles actually represent a hole (a caldera), not a peak. Let’s add colour by indicating the “variable” &lt;code&gt;..height..&lt;/code&gt; in the &lt;code&gt;colour&lt;/code&gt; aesthetic of &lt;code&gt;geom_cotour()&lt;/code&gt;, which will also indicate height as a legend:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(x=x, y=y)
dat$z &amp;lt;- as.vector(z)/2   # &amp;quot;De-exaggerate&amp;quot; the relief
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z, colour=..level..)) +
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank()) +
    scale_color_continuous(&amp;quot;Altitude&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can tell that the highest point is within the lightest blue area, to the left of the caldera.&lt;/p&gt;
&lt;p&gt;Now let’s add more contour lines, to get a better sense of the terrain. Do so by indicating the altitudes to make contours for via &lt;code&gt;breaks&lt;/code&gt;. Let’s make 5 unit spacing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(x=x, y=y)
dat$z &amp;lt;- as.vector(z)/2   # &amp;quot;De-exaggerate&amp;quot; the relief
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z, colour=..level..),
                 breaks=seq(100, 200, by=5)) +
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank()) +
    scale_color_continuous(&amp;quot;Altitude&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/contour_plots_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although you can change the contours, it’s best practice to keep the (height) spacing between contour lines equal – otherwise, the contour plot becomes harder to read. In the above plot, for example, we know that crossing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; contour lines (that are either increasing or decreasing) results in &lt;span class=&#34;math inline&#34;&gt;\(5n\)&lt;/span&gt; units of elevation gain/loss, because the spacing between contours is always 5 units.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mixture distributions</title>
      <link>/post/mixture_distributions/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/mixture_distributions/</guid>
      <description>


&lt;p&gt;This tutorial introduces the concept of a &lt;em&gt;mixture distribution&lt;/em&gt;. We’ll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.&lt;/p&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intuition&lt;/h2&gt;
&lt;p&gt;Let’s start by looking at a basic experiment:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Flip a coin.&lt;/li&gt;
&lt;li&gt;If the outcome is heads, generate a &lt;code&gt;N(0,1)&lt;/code&gt; random variable. If the outcome is tails, generate a &lt;code&gt;N(4,1)&lt;/code&gt; random variable. We’ll let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; denote the final result.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable with some distribution (spoiler: it’s a mixture distribution). Let’s perform the experiment 1000 times to get 1000 realizations of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and make a histogram to get a sense of the distribution &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; follows. To make sure the histogram represents an estimate of the density, we’ll make sure the area of the bars add to 1 (with the &lt;code&gt;..density..&lt;/code&gt; option).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(ggplot2))
set.seed(44)
X &amp;lt;- numeric(0)
coin &amp;lt;- integer(0)
for (i in 1:1000) {
    coin[i] &amp;lt;- rbinom(1, size=1, prob=0.5)  # flip a coin. 0=heads, 1=tails.
    if (coin[i] == 0) {   # heads
        X[i] &amp;lt;- rnorm(1, mean=0, sd=1)
    } else {           # tails
        X[i] &amp;lt;- rnorm(1, mean=4, sd=1)
    }
}
(p &amp;lt;- qplot(X, ..density.., geom=&amp;quot;histogram&amp;quot;, bins=30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mixture_distributions_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s try to reason our way to figuring out the overall density. Keep in mind that this density (like all densities) is &lt;em&gt;one curve&lt;/em&gt;. We’ll say we’ve succeeded at finding the density if our density is close to the histogram.&lt;/p&gt;
&lt;p&gt;It looks like the histogram is made up of two normal distributions “superimposed”. These ought to be related to the &lt;code&gt;N(0,1)&lt;/code&gt; and &lt;code&gt;N(4,1)&lt;/code&gt; distributions, so to start, let’s plot these two Gaussian densities overtop of the histogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(X=X), aes(X)) +
    geom_histogram(aes(y=..density..), bins=30) +
    stat_function(fun=function(x) dnorm(x, mean=0, sd=1), 
                  mapping=aes(colour=&amp;quot;Heads&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=4, sd=1), 
                  mapping=aes(colour=&amp;quot;Tails&amp;quot;)) +
    scale_color_discrete(&amp;quot;Coin Flip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mixture_distributions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, the two Gaussian distributions are in the correct location, and it even looks like they have the correct spread, but they’re too tall.&lt;/p&gt;
&lt;p&gt;Something to note at this point: the two curves plotted above are &lt;em&gt;separate (component) distributions&lt;/em&gt;. We’re trying to figure out the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; – which, again, is a single curve, and is estimated by the histogram. At this point, we only suspect that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is some combination of these two Gaussian distributions.&lt;/p&gt;
&lt;p&gt;So, why are the Gaussian curves too tall? Because each one represents the distribution &lt;em&gt;if we only ever flip either heads or tails&lt;/em&gt; (for example, the red distribution happens when we only ever flip heads). But since we flip heads half of the time, and tails half of the time, these probabilities (more accurately, densities) ought to be reduced by half. Let’s add these “semi” component distributions to the plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p &amp;lt;- ggplot(data.frame(X=X), aes(X)) +
    geom_histogram(aes(y=..density..), bins=30) +
    stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5, 
                  mapping=aes(colour=&amp;quot;Heads&amp;quot;, linetype=&amp;quot;Semi&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=4, sd=1)*0.5, 
                  mapping=aes(colour=&amp;quot;Tails&amp;quot;, linetype=&amp;quot;Semi&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=0, sd=1), 
                  mapping=aes(colour=&amp;quot;Heads&amp;quot;, linetype=&amp;quot;Full&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=4, sd=1), 
                  mapping=aes(colour=&amp;quot;Tails&amp;quot;, linetype=&amp;quot;Full&amp;quot;)) +
    scale_color_discrete(&amp;quot;Coin Flip&amp;quot;) +
    scale_linetype_discrete(&amp;quot;Distribution&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mixture_distributions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like they line up quite nicely!&lt;/p&gt;
&lt;p&gt;But these two curves are still separate – we need &lt;em&gt;one&lt;/em&gt; overall curve if we are to find the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. So we need to combine them somehow. It might look at first that we can just take the upper-most of the ‘semi’ curves (i.e., the maximum of the two), but looking in between the two curves reveals that the histogram is actually larger than either curve here. It turns out that the two ‘semi’ curves are &lt;em&gt;added&lt;/em&gt; to get the final curve:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5 + 
                      dnorm(x, mean=4, sd=1)*0.5,
                  mapping=aes(linetype=&amp;quot;Full&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/mixture_distributions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The intuition behind adding the densities is that an outcome for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; comes from &lt;em&gt;both&lt;/em&gt; components, so both contribute some density.&lt;/p&gt;
&lt;p&gt;Even though the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is made up of two components, at the end of the day, it’s still overall just a random variable with some density. And like all densities, the density of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is just one curve. But, this density happens to be &lt;em&gt;made up of&lt;/em&gt; the components, as we’ll see next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-scenario&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General Scenario&lt;/h2&gt;
&lt;p&gt;The two normal distributions from above are called &lt;em&gt;component distributions&lt;/em&gt;. In general, we can have any number of these (not just two) to make a mixture distribution. And, instead of selecting the component distribution with coin tosses, they’re chosen according to some generic probabilities called the &lt;em&gt;mixture probabilities&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In general, here’s how we make a mixture distribution with &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; component Gaussian distributions with densities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1(x), \ldots, \phi_K(x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose one of the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; components, randomly, with mixture probabilities &lt;span class=&#34;math inline&#34;&gt;\(\pi_1, \ldots, \pi_K\)&lt;/span&gt; (which, by necessity, add to 1).&lt;/li&gt;
&lt;li&gt;Generate a random variable from the selected component distribution. Call the result &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: we can use more than just Gaussian component distributions! But this tutorial won’t demonstrate that.&lt;/p&gt;
&lt;p&gt;That’s how we &lt;em&gt;generate&lt;/em&gt; a random variable with a mixture distribution, but what’s its density? We can derive that by the law of total probability. Let &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; be the selected component number; then the component distributions are actually the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;em&gt;conditional&lt;/em&gt; on the component number. We get:
&lt;span class=&#34;math display&#34;&gt;\[ f_X\left(x\right) = \sum_{k=1}^{K} f_{X|C}\left(x \mid c\right) P\left(C=c\right) = \sum_{k=1}^{K} \phi_k\left(x\right) \pi_k. \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Notes:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The intuition described in the previous section matches up with this result. For &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt; components determined by a coin toss &lt;span class=&#34;math inline&#34;&gt;\((\pi_1=\pi_2=0.5),\)&lt;/span&gt; we have
&lt;span class=&#34;math display&#34;&gt;\[ f_X\left(x\right) = \phi\left(x\right)0.5 + \phi\left(x-4\right)0.5, \]&lt;/span&gt;
which is the black curve in the previous plot.&lt;/li&gt;
&lt;li&gt;This tutorial works with univariate data. But mixture distributions can be multivariate, too. A &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-variate mixture distribution can be made by replacing the component distributions with &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-variate distributions. Just be sure to distinguish between the dimension of the data &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the number of components &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We &lt;em&gt;could&lt;/em&gt; just describe a mixture distribution by its density, just like we can describe a normal distribution by its density. But, describing mixture distributions by its &lt;strong&gt;component distributions&lt;/strong&gt; together with the &lt;strong&gt;mixture probabilities&lt;/strong&gt;, we obtain an excellent &lt;em&gt;interpretation&lt;/em&gt; of the mixture distribution. This interpretation is (it’s also called a &lt;em&gt;data generating process&lt;/em&gt;): (1) randomly choose a component, and (2) generate from that component. This interpretation is useful for cluster analysis, because the data clusters can be thought of as being generated by the component distributions, and the proportion of data in each cluster is determined by the mixture probabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Points&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A mixture distribution can be described by its mixing probabilities &lt;span class=&#34;math inline&#34;&gt;\(\pi_1, \ldots, \pi_K\)&lt;/span&gt; and component distributions &lt;span class=&#34;math inline&#34;&gt;\(\phi_1(x), \ldots, \phi_K(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A mixture distribution can also be described by a single density (like all continuous random variables).
&lt;ul&gt;
&lt;li&gt;This density is a single curve if data are univariate; a single “surface” if the data are bivariate; and higher dimensional surfaces if the data are higher dimensional.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;To get the density from the mixing probabilities and component distributions, we can use the formula indicated in the above section (based on the law of total probability).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Communicating Data</title>
      <link>/post/communicating_data_landing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/communicating_data_landing/</guid>
      <description>


&lt;p&gt;How to communicate data through visuals. Inspired by Claus Wilke’s &lt;a href=&#34;https://clauswilke.com/dataviz&#34;&gt;“Fundamentals of Data Visualization”&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vincenzocoia.com/communicating_data&#34;&gt;Slides here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generalized Additive Models</title>
      <link>/post/gam_in_r/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/gam_in_r/</guid>
      <description>


&lt;div id=&#34;generalized-additive-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalized Additive Models&lt;/h2&gt;
&lt;p&gt;To fit a GAM in R, we could use:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the function &lt;code&gt;gam&lt;/code&gt; in the &lt;code&gt;mgcv&lt;/code&gt; package, or&lt;/li&gt;
&lt;li&gt;the function &lt;code&gt;gam&lt;/code&gt; in the &lt;code&gt;gam&lt;/code&gt; package.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Differences between the two functions are discussed in the “Details” section of the &lt;code&gt;gam&lt;/code&gt; documentation in the &lt;code&gt;mgcv&lt;/code&gt; package. Choose one, but don’t load both! &lt;code&gt;mgcv&lt;/code&gt; tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that’s what is used in this tutorial. But the &lt;code&gt;gam&lt;/code&gt; package has similar workings.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;gam&lt;/code&gt; function works similarly to other regression functions, but the formula specification is different. Let’s go through different formula specifications, doing regression on the &lt;code&gt;mtcars&lt;/code&gt; dataset in R.&lt;/p&gt;
&lt;p&gt;The formula &lt;code&gt;mpg ~ disp + wt&lt;/code&gt; gives you a &lt;em&gt;linear model&lt;/em&gt;. It indicates that &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;wt&lt;/code&gt; both enter the model in a linear fashion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mgcv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nlme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mgcv 1.8-31. For overview type &amp;#39;help(&amp;quot;mgcv-package&amp;quot;)&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- gam(mpg ~ disp + wt, data=mtcars)
fit2 &amp;lt;- lm(mpg ~ disp + wt, data=mtcars)
summary(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ disp + wt
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 34.96055    2.16454  16.151 4.91e-16 ***
## disp        -0.01773    0.00919  -1.929  0.06362 .  
## wt          -3.35082    1.16413  -2.878  0.00743 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## R-sq.(adj) =  0.766   Deviance explained = 78.1%
## GCV = 9.3863  Scale est. = 8.5063    n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4087 -2.3243 -0.7683  1.7721  6.3484 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 34.96055    2.16454  16.151 4.91e-16 ***
## disp        -0.01773    0.00919  -1.929  0.06362 .  
## wt          -3.35082    1.16413  -2.878  0.00743 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.917 on 29 degrees of freedom
## Multiple R-squared:  0.7809, Adjusted R-squared:  0.7658 
## F-statistic: 51.69 on 2 and 29 DF,  p-value: 2.744e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the coefficient estimates are the same.&lt;/p&gt;
&lt;p&gt;To make a term non-parametric, wrap the &lt;code&gt;s&lt;/code&gt; function around the term (for &lt;em&gt;splines&lt;/em&gt;; comes with the &lt;code&gt;mgcv&lt;/code&gt; package). The &lt;code&gt;gam&lt;/code&gt; package also has a &lt;code&gt;lo&lt;/code&gt; function, for &lt;em&gt;loess&lt;/em&gt; smoothing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- gam(mpg ~ s(disp) + s(wt), data=mtcars)
summary(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ s(disp) + s(wt)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  20.0906     0.3429   58.59   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F  p-value    
## s(disp) 6.263  7.386 6.373 0.000164 ***
## s(wt)   1.000  1.000 4.015 0.056434 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.896   Deviance explained = 92.1%
## GCV = 5.0715  Scale est. = 3.762     n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, each predictor enters the model in a non-parametric, additive form. The nonparametric functions can be accessed by calling &lt;code&gt;plot&lt;/code&gt;. For documentation, see &lt;code&gt;?plot.gam&lt;/code&gt;. Let’s plot the “bivariate” scatterplots behind these curves too (these bivariate data actually use partial residuals).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit3, residuals=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gam_in_r_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/gam_in_r_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the “weight” variable (&lt;code&gt;wt&lt;/code&gt;) is quite linear. We can let it be linear, while the &lt;code&gt;disp&lt;/code&gt; variable remains nonparametric. “Wiggliness” of the smoothed fit can be controlled through the &lt;code&gt;k&lt;/code&gt; argument of the &lt;code&gt;s&lt;/code&gt; function, but this is chosen in a “smart” way by default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;- gam(mpg ~ s(disp, k=3) + wt, data=mtcars)
summary(fit4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ s(disp, k = 3) + wt
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  31.1110     3.1336   9.928  1.1e-10 ***
## wt           -3.4254     0.9649  -3.550  0.00138 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##          edf Ref.df     F  p-value    
## s(disp) 1.93  1.995 9.724 0.000758 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.839   Deviance explained = 85.4%
## GCV =  6.659  Scale est. = 5.8413    n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit4, residuals=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gam_in_r_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can even combine predictors into a common smooth function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit5 &amp;lt;- gam(mpg ~ s(disp, qsec) + wt, data=mtcars)
summary(fit5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ s(disp, qsec) + wt
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   32.181      4.340   7.415 1.83e-07 ***
## wt            -3.758      1.345  -2.794   0.0105 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                edf Ref.df     F  p-value    
## s(disp,qsec) 7.634  9.694 5.353 0.000312 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.903   Deviance explained =   93%
## GCV = 5.0335  Scale est. = 3.5181    n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each, the &lt;code&gt;predict&lt;/code&gt; and &lt;code&gt;residuals&lt;/code&gt; functions work in the same old way. Let’s use them to make a residual plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(predict(fit5), residuals(fit5)) +
    geom_abline(intercept=0, slope=0, linetype=&amp;quot;dashed&amp;quot;) +
    xlab(&amp;quot;Prediction (mean)&amp;quot;) + ylab(&amp;quot;Residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/gam_in_r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For their documentation, see &lt;code&gt;?predict.gam&lt;/code&gt; and &lt;code&gt;?residuals.gam&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>JAGS Tutorial</title>
      <link>/post/jags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/jags/</guid>
      <description>


&lt;p&gt;JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.&lt;/p&gt;
&lt;p&gt;You’ll need to &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;download and install JAGS&lt;/a&gt;. You can interact with JAGS through one of three R packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;runjags&lt;/code&gt; (recommended for this course)
&lt;ul&gt;
&lt;li&gt;Model written as a single string in R; possibly also allows you to input from file.&lt;/li&gt;
&lt;li&gt;Quick-start guide vignette: &lt;code&gt;vignette(&#34;quickjags&#34;, package=&#34;runjags&#34;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Full user guide vignette: &lt;code&gt;vignette(&#34;UserGuide&#34;, package=&#34;runjags&#34;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rjags&lt;/code&gt; (sample code &lt;a href=&#34;http://www.johnmyleswhite.com/notebook/2010/08/20/using-jags-in-r-with-the-rjags-package/&#34;&gt;here&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Model read in from plain text file.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;R2jags&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;this is dependent on &lt;code&gt;R2winbugs&lt;/code&gt;, which I find doesn’t work well outside of Windows machines, so I’m more hesitant to use this package.&lt;/li&gt;
&lt;li&gt;Model written in R as a function, but using JAGS language; or inputted from file.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, the &lt;code&gt;coda&lt;/code&gt; package is useful for working with the output of at least &lt;code&gt;runjags&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;About the JAGS language:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generates samples of parameters based on the prior and statistical model.&lt;/li&gt;
&lt;li&gt;Need to specify which parameters you want to include in the output (aka “track” or “monitor”).&lt;/li&gt;
&lt;li&gt;Specify probability distributions similarly to R, except:
&lt;ul&gt;
&lt;li&gt;Draw samples using calls like &lt;code&gt;dexp&lt;/code&gt; and &lt;code&gt;dnorm&lt;/code&gt;, not &lt;code&gt;rexp&lt;/code&gt; and &lt;code&gt;rnorm&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The JAGS version of &lt;code&gt;rnorm&lt;/code&gt; uses the precision (=1/variance) instead of standard deviation.
The documentation of JAGS code is not as nice as R. You have to look things up from a table-of-contents-style search from &lt;a href=&#34;http://www.stats.ox.ac.uk/~nicholls/MScMCMC15/jags_user_manual.pdf&#34;&gt;this&lt;/a&gt; document. Page 29 shows the aliases for various distributions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this week’s lab assignment (3), you’ll only be using it to generate observations from a distribution. Let’s generate data from a N(0,2) distribution (that is, variance=2), and ignore the warning messages for this week.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(runjags))
my_model &amp;lt;- &amp;quot;
model{
    # This is a comment.
    theta ~ dnorm(0, 1/2)
}
&amp;quot;
fit &amp;lt;- run.jags(my_model, monitor=&amp;quot;theta&amp;quot;, n.chains=1, sample=1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: No data was specified or found in the model file so the simulation was
## run withut data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calling the simulation...
## Welcome to JAGS 4.3.0 on Tue Sep 22 22:14:03 2020
## JAGS is free software and comes with ABSOLUTELY NO WARRANTY
## Loading module: basemod: ok
## Loading module: bugs: ok
## . . Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 0
##    Unobserved stochastic nodes: 1
##    Total graph size: 5
## . Initializing model
## . Adaptation skipped: model is not in adaptive mode.
## . Updating 4000
## -------------------------------------------------| 4000
## ************************************************** 100%
## . . Updating 1000
## -------------------------------------------------| 1000
## ************************************************** 100%
## . . . . Updating 0
## . Deleting model
## . 
## Note: the model did not require adaptation
## Simulation complete.  Reading coda files...
## Coda files loaded successfully
## Calculating summary statistics...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Convergence cannot be assessed with only 1 chain&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Finished running the simulation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theta &amp;lt;- coda::as.mcmc(fit)
head(theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Markov Chain Monte Carlo (MCMC) output:
## Start = 5001 
## End = 5007 
## Thinning interval = 1 
##          theta
## 5001  0.412988
## 5002 -1.439260
## 5003  0.520555
## 5004  1.702830
## 5005  0.840491
## 5006  1.610820
## 5007  1.567480&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/jags_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;more-sample-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More sample code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## x tidyr::extract() masks runjags::extract()
## x dplyr::filter()  masks stats::filter()
## x dplyr::lag()     masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(runjags)
n &amp;lt;- 50
dat &amp;lt;- tibble(x=rnorm(n),
              y=x + rnorm(n))
jagsdat &amp;lt;- c(as.list(dat), n=nrow(dat))
model &amp;lt;- &amp;quot;model{
    for (i in 1:n) {
        y[i] ~ dnorm(beta*x[i], tau)
    }
    tau &amp;lt;- pow(sigma, -2)
    sigma ~ dunif(0, 100)
    beta ~ dnorm(0, 0.001)
}&amp;quot;
foo &amp;lt;- run.jags(
    model=model, 
    monitor=c(&amp;quot;beta0&amp;quot;, &amp;quot;beta1&amp;quot;, &amp;quot;sigma&amp;quot;), 
    data=jagsdat
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: No initial value blocks found and n.chains not specified: 2 chains were
## used&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: No initial values were provided - JAGS will use the same initial values
## for all chains&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calling the simulation...
## Welcome to JAGS 4.3.0 on Tue Sep 22 22:14:07 2020
## JAGS is free software and comes with ABSOLUTELY NO WARRANTY
## Loading module: basemod: ok
## Loading module: bugs: ok
## . . Reading data file data.txt
## . Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 50
##    Unobserved stochastic nodes: 2
##    Total graph size: 159
## . Initializing model
## . Adapting 1000
## -------------------------------------------------| 1000
## ++++++++++++++++++++++++++++++++++++++++++++++++++ 100%
## Adaptation successful
## . Updating 4000
## -------------------------------------------------| 4000
## ************************************************** 100%
## . Failed to set trace monitor for beta0
## Variable beta0 not found
## . Failed to set trace monitor for beta1
## Variable beta1 not found
## . . Updating 10000
## -------------------------------------------------| 10000
## ************************************************** 100%
## . . . . . Updating 0
## . Deleting model
## . 
## Simulation complete.  Reading coda files...
## Coda files loaded successfully
## Calculating summary statistics...
## Calculating the Gelman-Rubin statistic for 1 variables....
## Finished running the simulation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(coda::as.mcmc(foo))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in as.mcmc.runjags(foo): Combining the 2 mcmc chains together&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/jags_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
