[{"authors":["admin"],"categories":null,"content":"Hi!\nI\u0026rsquo;m an Assistant Professor of Teaching at the University of British Columbia, Vancouver. My priority there is to establish the responsible use of statistics for data science.\nMy hobbies include outdoor recreation üå≤ such as hiking üëû, birding üê¶, skiing üéø, fishing üé£, canoeing üö£‚Äç‚ôÇÔ∏è, and others.\nSome of the things I do:\n I solve problems using statistics, machine learning, and data visualizations. I interact with students to make data science more approachable. I use R, git, and friends to help me do these things.  I can also identify birds, and other things about nature.\nHere are generic slides I use to introduce myself.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ed4d2245720b2d43a173bdec4161ed6","permalink":"/authors/vincenzo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vincenzo/","section":"authors","summary":"Hi!\nI\u0026rsquo;m an Assistant Professor of Teaching at the University of British Columbia, Vancouver. My priority there is to establish the responsible use of statistics for data science.\nMy hobbies include outdoor recreation üå≤ such as hiking üëû, birding üê¶, skiing üéø, fishing üé£, canoeing üö£‚Äç‚ôÇÔ∏è, and others.\nSome of the things I do:\n I solve problems using statistics, machine learning, and data visualizations. I interact with students to make data science more approachable.","tags":null,"title":"Vincenzo Coia","type":"authors"},{"authors":null,"categories":null,"content":" Work Experience 2017/01 - present\nLecturer of Data Science\n(Initially: Postdoctoral Teaching and Learning Fellow)\nMasters of Data Science Program, Department of Statistics\nThe University of British Columbia\nVancouver, BC\n2019/05 - 2019/08\nPart-time Flood Forecasting Consultant\nBGC Engineering\nVancouver, BC\n2009/05 - 2014/05\nShort-term statistical consulting (6 projects)\nUBC and private\n Education PhD in Statistics\n2012/09 - 2017/02\nThe University of British Columbia\nConferred May 29, 2017\nMSc in Mathematics and Statistics (Statistics)\n2011/09 - 2012/08\nBrock University\nConferred on October 13, 2012\nBSc (3-year) in Biological Sciences\nMinor in Earth Sciences\n2005/09 - 2011/04\nBrock University\nConferred ‚ÄúWith Distinction‚Äù on October 22, 2011\nBSc (Honours) Mathematics Integrated with Computers and Applications\nConcentration in Statistics\n2005/09 - 2011/04\nBrock University\nConferred ‚ÄúWith First-Class Standing‚Äù on June 7, 2011\n Teaching Course Involvement The following is a table of courses that I‚Äôve been either lecture or lab instructor for. They all took place at UBC.\n    Course Title Involvement    BAIT 509 Business Applications of Machine Learning Instructor, 2 years  STAT 545A Exploratory Data Analysis, Part I Instructor, 3 years  STAT 547M Exploratory Data Analysis, Part II Instructor, 2 years  DSCI 511 Programming for Data Science Lecture and Lab instructor, 2 years  DSCI 531 Data Visualization I Lecture instructor, 2 years; lab instructor, 1 year  DSCI 562 Regression II Lecture Instructor, 1 year; Lab Instructor, 3 years  DSCI 551 Probability for Data Science Lecture Instructor, 1 year  DSCI 561 Regression I Lab Instructor, 1 year  DSCI 571 Supervised Learning I Lab Instructor, 1 year  DSCI 532 Data Visualization II Lab Instructor, 2 years  DSCI 573 Feature and Model Selection Lab Instructor, 1 year  DSCI 563 Unsupervised Learning Lab Instructor, 1 year  DSCI 553 Statistical Inference and Computation II (Bayesian Statistics) Lab Instructor, 1 year  DSCI 574 Spatial and Temporal Models Lab Instructor, 2 years  DSCI 554 Experimentation and Causal Inference Lab Instructor, 2 years  DSCI 591 MDS Capstone Project Supervised 8 student teams over 3 years     Course (Re-) Developments I‚Äôve been involved with the redevelopment of the following courses:\n DSCI 561: Here, I lead a team and supervised one graduate student to redevelop the labs. DSCI 562: Both lectures and labs DSCI 531: The lectures one year, then the labs the following year. DSCI 532: Here, I lead a team and supervised two TA‚Äôs to redevelop the labs. DSCI 574: The labs.  I‚Äôve been involved with the development of the following courses:\n DSCI 591: I helped develop the capstone course with my colleagues. BAIT 509: I used DSCI 571 as inspiration for developing this course.  Some courses have been partially redeveloped.\n STAT 545A/547M: Some redevelopment of content; much redevelopment on course website and infrastructure. DSCI 551: Some redevelopment of lectures and lab to accommodate a problem-first approach. DSCI 554: The labs, after some shuffling of content to and from other parts of MDS.   Teaching Assistantships Duration: From the latter part of my undergrad, to the end of my PhD.\nUBC:\n SCIE 300: Communicating Science (5x)  Brock University:\n MATH 4P82/5P82: Non-parametric Statistics MATH 3P82: Regression Analysis MATH 4P81/5P81: Sampling Theory MATH 3P81: Experimental Design (2x) MATH 2F40: Mathematics Integrated w/ Computers and Applications II    Volunteer Positions 2016/09 - 2016/02\nScience World at TELUS World of Science \nVancouver, BC\n78.15 hours\n2013/10 - 2014/05\nBeaty Biodiversity Museum: Events Volunteer\nVancouver, BC\n35.0 hours\n2013/04 - 2013/09\nUBC Farm\nVancouver, BC\n102.5 hours\n2011/06 - 2011/08\nProject S.H.A.R.E. community garden\nNiagara Falls, ON\n15.0 hours\n Research Assistantships 2013/05 - 2013/08\nRobust penalized regression\nSupervisor: Dr.¬†Gabriela Cohen-Freue\nDepartment of Statistics\nThe University of British Columbia\nVancouver, BC\n2012/05 - 2012/08\n2011/05 - 2011/08\n2010/05 - 2010/08\nExtreme value modelling\nSupervisor: Dr.¬†Mei Ling Huang\nDepartment of Mathematics\nBrock University\nSt.¬†Catharines, ON\n2010/09 - 2011/06\nQuantum monte carlo simulations\nSupervisors: Dr.¬†Stuart Rothstein; Dr.¬†Wai Kong (John) Yuen\nDepartment of Chemistry and Department of Mathematics\nBrock University\nSt.¬†Catharines, ON\n Publications and Talks Articles in Preparation  Coia, V., Joe, H., and Nolde, N. (2020?) Extreme Quantile Regression and the Copula-Marginal Composition Method. Status: Wrapping up the demonstration analysis   Articles Submitted to Refereed Journals  Huang, M.L., Coia, V., and Brill, P.H. (2013) A cluster truncated Pareto distribution and its applications. ISRN Probability and Statistics 2013: Article ID 265373.\n Ayad, M., Coia, V., and Kihel, O. (2014) The number of relatively prime subsets of a finite union of sets of consecutive integers. Journal of Integer Sequences 17: Article 14.3.7\n Coia, V., and Huang, M.L. (2014) A sieve model for extreme values. Journal of Statistical Computation and Simulation. 84(8):16921710.\n   Articles Submitted to Conference Proceedings  Huang, M. L., Coia, V., and Brill, P.H., A mixture truncated Pareto distribution, In JSM Proceedings 2012, Statistical Computing Section, Alexandria, VA: American Statistical Association, pp.     Conference and Roundtable Contributions  Coia, V. ‚ÄúThe squared error has friends, too!‚Äù (Contributed Talk). SFU/UBC Joint Graduate Student Seminar (Winter). Spring 2019 at the SFU Harbour Centre, Vancouver, BC.\n Coia, V., Nolde, N., and Joe, H. Forecasting Extremes for Flooding (Invited Talk). The 44th Annual Meeting of the Statistical Society of Canada. May 29June 1, 2016 at Brock University, St.¬†Catharines, ON.\n Coia, V., and Jeanniard du Dot, T. (Invited Demonstration) ‚ÄúUsing the Grammar of Graphics and Interactivity to explore Biologging Data in R‚Äù. May 6, 2015. Building a Bioanalytical Theory for Analysis of Marine Mammal Movements: A Peter Wall International Research Roundtable. The University of British Columbia, Vancouver, BC.\n Coia, V. ‚ÄúFlood Warning: An Application of High-Quantile Regression‚Äù (Contributed Talk). SFU/UBC Joint Graduate Student Seminar (Winter). February 28, 2015 at the SFU Harbour Centre, Vancouver, BC.\n Coia, V. ‚ÄúA New Sieve Model for Extreme Values‚Äù (Contributed Talk). SFU/UBC Joint Graduate Student Seminar (Fall). September 29, 2012 at the SFU Harbour Centre, Vancouver, BC.\n Coia, V., and Huang, M.L. ‚ÄúOn Estimation of Heavy Tailed Distributions‚Äù (Contributed Talk). The 40th Annual Meeting of the Statistical Society of Canada. June 36, 2012 at the University of Guelph, Guelph, ON.\n Huang, M.L., Coia, V., and Brill, P.H. ‚ÄúA Mixture Truncated Pareto Distribution‚Äù (Contributed Talk). The 2012 Joint Statistical Meetings. July 28August 2, 2012 at San Diego, California\n    Awards University Issued  2012/09 - 2016/08: Four-Year Fellowship 2012/09 - 2016/08: Faculty of Science Graduate Award 2011/09: Dean of Graduate Studies Excellence Scholarship 2011/06/07: Dean‚Äôs Gold Medal 2011/06/07: Distinguished Undergraduate Student Award in Mathematics 2011/03: President‚Äôs Surgite Award   Nationally Recognized  2013/06: Governor General of Canada‚Äôs Gold Medal 2012/09 - 2015/08: NSERC Postgraduate Award (Doctoral, 3-year) 2011/09 - 2012/08: NSERC Alexander Graham Bell Canada Graduate Scholarship (Masters) 2010/05 - 2010/08: NSERC Undergraduate Student Research Award   Individual Donors  2012/04: Dr.¬†Jack Lightstone \u0026amp; Dorothy Markiewicz Scholarship 2012/04: Dr.¬†Raymond \u0026amp; Mrs.¬†Sachi Moriyama Grad. Fellowship 2012/03: Tomlinson Entrance Scholarship for Excellence in Mathematics and Science 2011/06: John and Roslyn Reed Book Prize 2010/09: Art Bicknell Scholarship in Mathematics 2010/09: Ian D. Beddis Family Scholarship 2010/09: Terry and Sue White Mathematics and Science Scholarship 2007/09: M.J. (‚ÄúMel‚Äù) Farquharson Scholarship 2006/09: Scholler Foundation Scholarship in Chemistry   Athletic Awards  2016/04/03: Sportsmanship Award, UBC Thunderbirds Sport Clubs (Fencing) 2016/02/28: Bronze Medal in Senior Mixed Epee (Intercollegiate Tournament, UBC) 2015/11/16: Bronze Medal in Senior Mixed Epee (Remembrance Day Tournament, UBC) 2012/03/28: RM Davis Surgite Award 2009/12/12: First Place Award Winter Epeedemic, Toronto Fencing Club 2009/03: Varsity Fencing Rookie of the Year Award   Declined Awards  2012/04: Ontario Graduate Scholarship (Doctoral) 2011/05: Ontario Graduate Scholarship (Masters)    Professional Activities Dept. of Statistics, The University of British Columbia:\n 2016/05 - 2016/06: Search committee member (for CRC 2 faculty position) 2015/04/30: Organizer of the ‚ÄúHow to Write an Awesome Abstract‚Äù workshop 2014/06 - 2015/05: Organizer of the SFU/UBC Joint Seminar 2014/06 - 2014/09: Organizer of the Graduate Student Trip 2014/04 - 2015/05: Co-founder of the Graduate Writing Forums 2013/06 - 2014/05: Statistics Graduate Student Representative   ","date":1600732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600732800,"objectID":"83a254add626bb2e90c041edccf73cf9","permalink":"/cv/","publishdate":"2020-09-22T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Work Experience 2017/01 - present\nLecturer of Data Science\n(Initially: Postdoctoral Teaching and Learning Fellow)\nMasters of Data Science Program, Department of Statistics\nThe University of British Columbia\nVancouver, BC\n2019/05 - 2019/08\nPart-time Flood Forecasting Consultant\nBGC Engineering\nVancouver, BC\n2009/05 - 2014/05\nShort-term statistical consulting (6 projects)\nUBC and private\n Education PhD in Statistics\n2012/09 - 2017/02\nThe University of British Columbia\nConferred May 29, 2017","tags":null,"title":"Vincenzo Coia's CV","type":"page"},{"authors":null,"categories":null,"content":"You all know the drill \u0026ndash; you\u0026rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?\nRegression trees, loess, linear regression\u0026hellip; you name it, they\u0026rsquo;re all in pursuit of the mean (well, almost all). But the true outcome is random. It has a distribution. Are you sure you want the mean of that distribution?\nYou might say \u0026ldquo;Yes! It ensures my prediction is as close as possible to the outcome!\u0026rdquo; If this is indeed what you want, the mean still might not be your best choice \u0026ndash; it only ensures the mean squared error is minimized.\nThere are a suite of other options that might be more appropriate than the mean. The good thing is, your favourite supervised learning method probably has a natural extension for estimating these alternatives. Let\u0026rsquo;s investigate the quantities you might care about.\nThe Median No, the median isn\u0026rsquo;t just an inferior version of the mean, to be used under the unfortunate presence of outliers.\nIf I randomly pick a data scientist, what do you think their salary would be? This distribution has a right-skew, so chances are, your data scientist earns less than the mean. Predict the median, and you\u0026rsquo;ll have a 50% chance that your data scientist does earn at least what you predict.\nIn short, use the median when you want your prediction to be exceeded with a coin toss.\nMinimize the mean absolute error to get this prediction.\nHigher (or lower) Quantiles Want to make it to an interview on time? You add some \u0026ldquo;buffer time\u0026rdquo; to the expected travel time, right? What you\u0026rsquo;re after is a high quantile of travel time \u0026ndash; something like the 0.99-quantile, so that there is only a small chance you\u0026rsquo;ll be late (1% in this case).\nUse a high (or low) quantile if you want a conservative (or liberal) prediction \u0026ndash; or both, if you want a prediction interval.\nMinimize the mean rho function to get this prediction.\nThe Mean The mean is useful when we care about totals. Want to know how much gas a vehicle uses? You\u0026rsquo;re after the mean, because the total quantity drawn out over time is what matters.\nMinimize the mean squared error to get this prediction.\nOther Options Do you really need to distill your prediction down to a single number? Consider looking at the entire distribution of the outcome as your prediction (typically conditional on predictors) \u0026ndash; after all, this conveys the entire uncertainty about the outcome. This is known as probabilistic forecasting.\nThere are other measures, too. Expected shortfall is useful for risk analysis, or even expectiles. Maybe you care about variance or skewness for some reason. Whatever you want to get at, just make sure you ask yourself what you actually care about. You have an entire distribution to distill!\n(Photo from Pexels)\n","date":1518912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518912000,"objectID":"fbd647593c6cde38a2379db9c9b91c86","permalink":"/post/2018-02-18-mean/","publishdate":"2018-02-18T00:00:00Z","relpermalink":"/post/2018-02-18-mean/","section":"post","summary":"You all know the drill \u0026ndash; you\u0026rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?\nRegression trees, loess, linear regression\u0026hellip; you name it, they\u0026rsquo;re all in pursuit of the mean (well, almost all). But the true outcome is random.","tags":["supervised learning","mean","quantile","median","probabilistic forecasting"],"title":"The missing question in supervised learning","type":"post"},{"authors":null,"categories":null,"content":" This tutorial introduces contour plots, and how to plot them in ggplot2.\nWhat is a contour plot? Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?\nThe idea is to use contour lines, which are curves that indicate a constant height.\nImagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude. Then trace (on your map) the shapes formed by the new (flat) mountain tops. These curves are contour lines. Choose a differential such as 50 meters, and draw these curves for altitudes ‚Ä¶800m, 850m, 900m, 950m, 1000m, ‚Ä¶ ‚Äì the result is a contour plot (or topographic map, if it‚Äôs a map).\nIn general, contour plots are useful for functions of two variables (like a bivariate gaussian density).\nWe‚Äôll look at examples in the next section.\nNotes on contours:\n They never cross. The steepest slope at a point is parallel to the contour line. They aren‚Äôt entirely ambiguous. For example, you can‚Äôt tell whether or not the mountains are actually mountains, or whether they‚Äôre holes/valleys! Sometimes you can add colour to indicate depth; other times (like in topographic maps) you can indicate elevation directly as numbers beside contour lines. Other times, this is not required, because the context makes it obvious.   Contour plots in ggplot2 There are two ways you can make contour plots in ggplot2 ‚Äì but they‚Äôre both for quite different purposes.\nMethod 1: Approximate a bivariate density This method approximates a bivariate density from data.\nFirst, recall how this is done in the univariate case. A little kernel function (like a shrunken bell curve) is placed over each data point, and these are added together to get a density estimate:\nlibrary(ggplot2) set.seed(373) x \u0026lt;- rnorm(1000) ggplot(data.frame(x=x), aes(x)) + geom_density() We can do the same thing to get a bivariate density, except with little bivariate kernel functions (like shrunken bivariate Gaussian densities). But, we can‚Äôt just simply put ‚Äúdensity height‚Äù on the vertical axis ‚Äì we need that for the second dimension. Instead, we can use contour plots.\nThis is the contour plot that ggplot2‚Äôs geom_density2d() does: builds a bivariate kernel density estimate (based on data), then makes a contour plot out of it:\ny \u0026lt;- rnorm(1000) ggplot(data.frame(x=x, y=y), aes(x, y)) + geom_density2d() Based on context (this is a density), we know that this is a ‚Äúhill‚Äù and not a ‚Äúhole‚Äù. If you were to start at some point at the ‚Äúbottom‚Äù of the hill, the steepest way up would be perpendicular to the contours. The highest point on the hill is within the middle-most circle.\n Method 2: General Contour Plots You can also make contour plots that aren‚Äôt a kernel density estimate (necessarily), using geom_contour(). This is based off of any bivariate function.\nBasics Suppose we want to make a contour plot of the bivariate function \\[f(x,y) = x^2 + sin(y)\\] over the rectangle \\(-2\u0026lt;x\u0026lt;2\\) and \\(-5\u0026lt;y\u0026lt;5\\). First, make a grid over the rectangle (it must be a grid ‚Äì geom_contour() won‚Äôt work otherwise). Then, evaluate the function at each of the grid points. Put all this info into a single data frame with three columns (two for the \\(x\\) and \\(y\\) coordinates, and one for the function evaluation). Then, indicate the function evaluation in geom_contour() as the aesthetic z, and the x and y aesthetics are as usual.\nf \u0026lt;- function(x) x[1]^2 + sin(x[2]) x \u0026lt;- seq(-2, 2, length.out=100) y \u0026lt;- seq(-5, 5, length.out=100) dat \u0026lt;- expand.grid(x=x, y=y) # Data frame of 100*100=10000 points. dat$z \u0026lt;- apply(dat, 1, f) ggplot(dat, aes(x, y)) + geom_contour(aes(z=z)) Notice that expand.grid is useful for making grids. It returns all pairs from the input vectors. But, this also means that it‚Äôs easy for the output to explode!\nNote that finer grids yield plots with higher accuracy. Here‚Äôs an example of a rough grid, whose contours are jagged:\nf \u0026lt;- function(x) x[1]^2 + sin(x[2]) x \u0026lt;- seq(-2, 2, length.out=10) y \u0026lt;- seq(-5, 5, length.out=10) dat \u0026lt;- expand.grid(x=x, y=y) # Data frame of 10*10=100 points. dat$z \u0026lt;- apply(dat, 1, f) ggplot(dat, aes(x, y)) + geom_contour(aes(z=z))  Additional Settings Here, we‚Äôll look at colouring the plots, and adding more/less contours.\nHere‚Äôs another example, with the volcano data (a matrix of altitudes for a volcano). If you‚Äôd like, first take a look at a 3D rendering of the volcano, by running the following code chunk in your R console after un-commenting the last two lines (code taken directly from rgl‚Äôs surface3d() documentation):\ndata(volcano) z \u0026lt;- 2 * volcano # Exaggerate the relief x \u0026lt;- 10 * (1:nrow(z)) # 10 meter spacing (S to N) y \u0026lt;- 10 * (1:ncol(z)) # 10 meter spacing (E to W) zlim \u0026lt;- range(y) zlen \u0026lt;- zlim[2] - zlim[1] + 1 colorlut \u0026lt;- terrain.colors(zlen) # height color lookup table col \u0026lt;- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point # open3d() # surface3d(x, y, z, color = col, back = \u0026quot;lines\u0026quot;) Feel free to move the image around by clicking and dragging. Neat, eh?\nWe‚Äôll make a contour plot with this.\ndat \u0026lt;- expand.grid(x=x, y=y) dat$z \u0026lt;- as.vector(z)/2 # \u0026quot;De-exaggerate\u0026quot; the relief ggplot(dat, aes(x, y)) + geom_contour(aes(z=z)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + theme(axis.text=element_blank(), axis.ticks=element_blank()) But, you can‚Äôt tell that the inner circles actually represent a hole (a caldera), not a peak. Let‚Äôs add colour by indicating the ‚Äúvariable‚Äù ..height.. in the colour aesthetic of geom_cotour(), which will also indicate height as a legend:\ndat \u0026lt;- expand.grid(x=x, y=y) dat$z \u0026lt;- as.vector(z)/2 # \u0026quot;De-exaggerate\u0026quot; the relief ggplot(dat, aes(x, y)) + geom_contour(aes(z=z, colour=..level..)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + theme(axis.text=element_blank(), axis.ticks=element_blank()) + scale_color_continuous(\u0026quot;Altitude\u0026quot;) Now we can tell that the highest point is within the lightest blue area, to the left of the caldera.\nNow let‚Äôs add more contour lines, to get a better sense of the terrain. Do so by indicating the altitudes to make contours for via breaks. Let‚Äôs make 5 unit spacing:\ndat \u0026lt;- expand.grid(x=x, y=y) dat$z \u0026lt;- as.vector(z)/2 # \u0026quot;De-exaggerate\u0026quot; the relief ggplot(dat, aes(x, y)) + geom_contour(aes(z=z, colour=..level..), breaks=seq(100, 200, by=5)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + theme(axis.text=element_blank(), axis.ticks=element_blank()) + scale_color_continuous(\u0026quot;Altitude\u0026quot;) Although you can change the contours, it‚Äôs best practice to keep the (height) spacing between contour lines equal ‚Äì otherwise, the contour plot becomes harder to read. In the above plot, for example, we know that crossing \\(n\\) contour lines (that are either increasing or decreasing) results in \\(5n\\) units of elevation gain/loss, because the spacing between contours is always 5 units.\n   ","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"1cde67e5128905fad57b7e912e8832de","permalink":"/post/contour_plots/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/contour_plots/","section":"post","summary":"This tutorial introduces contour plots, and how to plot them in ggplot2.\nWhat is a contour plot? Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?\nThe idea is to use contour lines, which are curves that indicate a constant height.\nImagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude.","tags":null,"title":"Contour Plots","type":"post"},{"authors":null,"categories":null,"content":" This tutorial introduces the concept of a mixture distribution. We‚Äôll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.\nIntuition Let‚Äôs start by looking at a basic experiment:\nFlip a coin. If the outcome is heads, generate a N(0,1) random variable. If the outcome is tails, generate a N(4,1) random variable. We‚Äôll let \\(X\\) denote the final result.  \\(X\\) is a random variable with some distribution (spoiler: it‚Äôs a mixture distribution). Let‚Äôs perform the experiment 1000 times to get 1000 realizations of \\(X\\), and make a histogram to get a sense of the distribution \\(X\\) follows. To make sure the histogram represents an estimate of the density, we‚Äôll make sure the area of the bars add to 1 (with the ..density.. option).\nsuppressMessages(library(ggplot2)) set.seed(44) X \u0026lt;- numeric(0) coin \u0026lt;- integer(0) for (i in 1:1000) { coin[i] \u0026lt;- rbinom(1, size=1, prob=0.5) # flip a coin. 0=heads, 1=tails. if (coin[i] == 0) { # heads X[i] \u0026lt;- rnorm(1, mean=0, sd=1) } else { # tails X[i] \u0026lt;- rnorm(1, mean=4, sd=1) } } (p \u0026lt;- qplot(X, ..density.., geom=\u0026quot;histogram\u0026quot;, bins=30)) Let‚Äôs try to reason our way to figuring out the overall density. Keep in mind that this density (like all densities) is one curve. We‚Äôll say we‚Äôve succeeded at finding the density if our density is close to the histogram.\nIt looks like the histogram is made up of two normal distributions ‚Äúsuperimposed‚Äù. These ought to be related to the N(0,1) and N(4,1) distributions, so to start, let‚Äôs plot these two Gaussian densities overtop of the histogram.\nggplot(data.frame(X=X), aes(X)) + geom_histogram(aes(y=..density..), bins=30) + stat_function(fun=function(x) dnorm(x, mean=0, sd=1), mapping=aes(colour=\u0026quot;Heads\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=4, sd=1), mapping=aes(colour=\u0026quot;Tails\u0026quot;)) + scale_color_discrete(\u0026quot;Coin Flip\u0026quot;) Well, the two Gaussian distributions are in the correct location, and it even looks like they have the correct spread, but they‚Äôre too tall.\nSomething to note at this point: the two curves plotted above are separate (component) distributions. We‚Äôre trying to figure out the distribution of \\(X\\) ‚Äì which, again, is a single curve, and is estimated by the histogram. At this point, we only suspect that the distribution of \\(X\\) is some combination of these two Gaussian distributions.\nSo, why are the Gaussian curves too tall? Because each one represents the distribution if we only ever flip either heads or tails (for example, the red distribution happens when we only ever flip heads). But since we flip heads half of the time, and tails half of the time, these probabilities (more accurately, densities) ought to be reduced by half. Let‚Äôs add these ‚Äúsemi‚Äù component distributions to the plot:\n(p \u0026lt;- ggplot(data.frame(X=X), aes(X)) + geom_histogram(aes(y=..density..), bins=30) + stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5, mapping=aes(colour=\u0026quot;Heads\u0026quot;, linetype=\u0026quot;Semi\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=4, sd=1)*0.5, mapping=aes(colour=\u0026quot;Tails\u0026quot;, linetype=\u0026quot;Semi\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=0, sd=1), mapping=aes(colour=\u0026quot;Heads\u0026quot;, linetype=\u0026quot;Full\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=4, sd=1), mapping=aes(colour=\u0026quot;Tails\u0026quot;, linetype=\u0026quot;Full\u0026quot;)) + scale_color_discrete(\u0026quot;Coin Flip\u0026quot;) + scale_linetype_discrete(\u0026quot;Distribution\u0026quot;)) Looks like they line up quite nicely!\nBut these two curves are still separate ‚Äì we need one overall curve if we are to find the distribution of \\(X\\). So we need to combine them somehow. It might look at first that we can just take the upper-most of the ‚Äòsemi‚Äô curves (i.e., the maximum of the two), but looking in between the two curves reveals that the histogram is actually larger than either curve here. It turns out that the two ‚Äòsemi‚Äô curves are added to get the final curve:\np + stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5 + dnorm(x, mean=4, sd=1)*0.5, mapping=aes(linetype=\u0026quot;Full\u0026quot;)) The intuition behind adding the densities is that an outcome for \\(X\\) comes from both components, so both contribute some density.\nEven though the random variable \\(X\\) is made up of two components, at the end of the day, it‚Äôs still overall just a random variable with some density. And like all densities, the density of \\(X\\) is just one curve. But, this density happens to be made up of the components, as we‚Äôll see next.\n General Scenario The two normal distributions from above are called component distributions. In general, we can have any number of these (not just two) to make a mixture distribution. And, instead of selecting the component distribution with coin tosses, they‚Äôre chosen according to some generic probabilities called the mixture probabilities.\nIn general, here‚Äôs how we make a mixture distribution with \\(K\\) component Gaussian distributions with densities \\(\\phi_1(x), \\ldots, \\phi_K(x)\\):\nChoose one of the \\(K\\) components, randomly, with mixture probabilities \\(\\pi_1, \\ldots, \\pi_K\\) (which, by necessity, add to 1). Generate a random variable from the selected component distribution. Call the result \\(X\\).  Note: we can use more than just Gaussian component distributions! But this tutorial won‚Äôt demonstrate that.\nThat‚Äôs how we generate a random variable with a mixture distribution, but what‚Äôs its density? We can derive that by the law of total probability. Let \\(C\\) be the selected component number; then the component distributions are actually the distribution of \\(X\\) conditional on the component number. We get: \\[ f_X\\left(x\\right) = \\sum_{k=1}^{K} f_{X|C}\\left(x \\mid c\\right) P\\left(C=c\\right) = \\sum_{k=1}^{K} \\phi_k\\left(x\\right) \\pi_k. \\]\nNotes:  The intuition described in the previous section matches up with this result. For \\(K=2\\) components determined by a coin toss \\((\\pi_1=\\pi_2=0.5),\\) we have \\[ f_X\\left(x\\right) = \\phi\\left(x\\right)0.5 + \\phi\\left(x-4\\right)0.5, \\] which is the black curve in the previous plot. This tutorial works with univariate data. But mixture distributions can be multivariate, too. A \\(d\\)-variate mixture distribution can be made by replacing the component distributions with \\(d\\)-variate distributions. Just be sure to distinguish between the dimension of the data \\(d\\) and the number of components \\(K\\). We could just describe a mixture distribution by its density, just like we can describe a normal distribution by its density. But, describing mixture distributions by its component distributions together with the mixture probabilities, we obtain an excellent interpretation of the mixture distribution. This interpretation is (it‚Äôs also called a data generating process): (1) randomly choose a component, and (2) generate from that component. This interpretation is useful for cluster analysis, because the data clusters can be thought of as being generated by the component distributions, and the proportion of data in each cluster is determined by the mixture probabilities.    Learning Points  A mixture distribution can be described by its mixing probabilities \\(\\pi_1, \\ldots, \\pi_K\\) and component distributions \\(\\phi_1(x), \\ldots, \\phi_K(x)\\). A mixture distribution can also be described by a single density (like all continuous random variables).  This density is a single curve if data are univariate; a single ‚Äúsurface‚Äù if the data are bivariate; and higher dimensional surfaces if the data are higher dimensional.  To get the density from the mixing probabilities and component distributions, we can use the formula indicated in the above section (based on the law of total probability).   ","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"5700d987d2a4eafe2a512a48d71a74b3","permalink":"/post/mixture_distributions/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/mixture_distributions/","section":"post","summary":"This tutorial introduces the concept of a mixture distribution. We‚Äôll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.\nIntuition Let‚Äôs start by looking at a basic experiment:\nFlip a coin. If the outcome is heads, generate a N(0,1) random variable. If the outcome is tails, generate a N(4,1) random variable. We‚Äôll let \\(X\\) denote the final result.","tags":null,"title":"Mixture distributions","type":"post"},{"authors":null,"categories":null,"content":"\u0026ldquo;Probabilistic Thinking in Data Science\u0026rdquo; Initiative Resources to migrate to this new resource.\nVision for how people will engage:\n As if they are learning something for a course. Perhaps a blog  Content Ideas  irreducible error is reducible (by adding predictors) the inappropriate use of the words \u0026ldquo;can/can\u0026rsquo;t\u0026rdquo; and \u0026ldquo;need\u0026rdquo; in statistics.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4de48ece2395eafcedb4b1ee38be6764","permalink":"/probabilistic-thinking/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/probabilistic-thinking/readme/","section":"probabilistic-thinking","summary":"\u0026ldquo;Probabilistic Thinking in Data Science\u0026rdquo; Initiative Resources to migrate to this new resource.\nVision for how people will engage:\n As if they are learning something for a course. Perhaps a blog  Content Ideas  irreducible error is reducible (by adding predictors) the inappropriate use of the words \u0026ldquo;can/can\u0026rsquo;t\u0026rdquo; and \u0026ldquo;need\u0026rdquo; in statistics.  ","tags":null,"title":"","type":"probabilistic-thinking"},{"authors":null,"categories":null,"content":"Our Curriculum, Part II This post is a continuation of our post from last summer on the role of computer science in the Master of Data Science (MDS) curriculum here at UBC-Vancouver. This time, we\u0026rsquo;re focussing on the role of Statistics in the program.\nWe see two main challenges in teaching Statistics in our program:\n Turning established statistical framework into a lens for data science. Fitting this into a 10-month program amongst other data science topics.  1. Statistics does not directly translate to data science Statistical science: to us, is the science of numerically describing uncertainty. But the things one talks about when discussing statistics depends on the end in mind.\nTypically and traditionally, the end involves establishing properties of a statistical method. For example, finding the asymptotic distributions of GLM estimates, or describing the usefulness of Pearson\u0026rsquo;s correlation in measuring dependence. These are method-first approaches, where we begin with a model (such as a GLM or two random variables with dependence), and explore its properties. A course in statistics that\u0026rsquo;s grouped by method tends to be indicative of a method-first approach: a lecture on the GLM starts by defining the GLM, then moves on to its properties and a description of how to implement it; a lecture on maximum likelihood estimation defines the likelihood function, the loss function, and the estimator\u0026rsquo;s asymptotic distribution.\nThe end for statistics for data science is a solution to a real-world problem, making this a problem-first approach to statistics. The method-first approach does not easily translate here. For example, how do we know when to use a GLM for a given problem? Sure, the method-first approach tells us what the data should look like before using a GLM, but the converse is not necessarily true: having data that looks like it fits a GLM does not mean that you should fit a GLM. A random forest might be better for your problem. Or, maybe it\u0026rsquo;s better to drop the Poisson assumption (yep, you can do that).\n2.\\ Statistics for Data Science To make a course in statistics with a problem-first approach, lectures should be grouped by (you guessed it) problem. As an example, our Regression II course has topics such as:\n Regression when the range of your response is restricted Regression when your data are censored Regression when some data are missing  In each scenario, we look at the pros and cons of approaches for two encompassing problem types: optimizing prediction, and gaining interpretation. We focus on pros and cons of making assumptions in two areas: the model function and the conditional distributions. For example, when the range of the response is restricted to being positive, we look at the value of a linear predictor with both the identity and log link functions, and the value of adding a Poisson assumption to the conditional distributions, to arrive at a GLM as one possible solution.\nIn our experience, statistics discussed from a problem-first approach is hard to come by, so we see this as an area deserving active research. Even applied statistics tends to take a method-first approach, the difference being a stronger focus on how to draw conclusions from a method on various problems. Or, sometimes applied statistics does start with a real problem, but then the bulk of the attention is spent developing a method, then describing its properties.\nHow to make a problem-first course We\u0026rsquo;ve found the following to be useful:\n Begin with a statistical method. Distill this down to a problem type by asking why one would want to use this method. Ask: What are the assumptions of the approach? Are they really \u0026ldquo;needed\u0026rdquo;?  Trimming To fit the concepts into a 10-month program, we use the principle that the tidyverse adopts, and that\u0026rsquo;s to establish a simple foundation that covers \u0026ldquo;most\u0026rdquo; situations. Sometimes, this means not discussing how a model is estimated (such as the proportional hazards model), but perhaps just giving a brief general idea in words.\nStat is still important before problem-first approach.\n Authors: Vincenzo Coia is a lecturer with the MDS Vancouver program in the UBC Department of Statistics.\nAcknowlegdements ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"247a80309218cc8a15779ee66f7f3e00","permalink":"/probabilistic-thinking/statistics-blog_post/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/probabilistic-thinking/statistics-blog_post/","section":"probabilistic-thinking","summary":"Our Curriculum, Part II This post is a continuation of our post from last summer on the role of computer science in the Master of Data Science (MDS) curriculum here at UBC-Vancouver. This time, we\u0026rsquo;re focussing on the role of Statistics in the program.\nWe see two main challenges in teaching Statistics in our program:\n Turning established statistical framework into a lens for data science. Fitting this into a 10-month program amongst other data science topics.","tags":null,"title":"","type":"probabilistic-thinking"},{"authors":null,"categories":null,"content":" How to communicate data through visuals. Inspired by Claus Wilke‚Äôs ‚ÄúFundamentals of Data Visualization‚Äù.\nSlides here\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fb8d7098d08281d389576aaba498fe15","permalink":"/post/communicating_data_landing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/communicating_data_landing/","section":"post","summary":"How to communicate data through visuals. Inspired by Claus Wilke‚Äôs ‚ÄúFundamentals of Data Visualization‚Äù.\nSlides here","tags":null,"title":"Communicating Data","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"57bf93109dcec65252926da1c8d0d700","permalink":"/project/coperate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/coperate/","section":"project","summary":"Draw powerful multivariate insights using copulas. R package. **Coming soon!**","tags":["R"],"title":"coperate","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ea2583cbcde5de3401ae36896d04eef5","permalink":"/project/distplyr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/distplyr/","section":"project","summary":"Draw powerful insights using distributions. R package.","tags":["R"],"title":"distplyr","type":"project"},{"authors":null,"categories":null,"content":" Generalized Additive Models To fit a GAM in R, we could use:\nthe function gam in the mgcv package, or the function gam in the gam package.  Differences between the two functions are discussed in the ‚ÄúDetails‚Äù section of the gam documentation in the mgcv package. Choose one, but don‚Äôt load both! mgcv tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that‚Äôs what is used in this tutorial. But the gam package has similar workings.\nThe gam function works similarly to other regression functions, but the formula specification is different. Let‚Äôs go through different formula specifications, doing regression on the mtcars dataset in R.\nThe formula mpg ~ disp + wt gives you a linear model. It indicates that disp and wt both enter the model in a linear fashion.\nlibrary(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-31. For overview type \u0026#39;help(\u0026quot;mgcv-package\u0026quot;)\u0026#39;. fit1 \u0026lt;- gam(mpg ~ disp + wt, data=mtcars) fit2 \u0026lt;- lm(mpg ~ disp + wt, data=mtcars) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ disp + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## ## R-sq.(adj) = 0.766 Deviance explained = 78.1% ## GCV = 9.3863 Scale est. = 8.5063 n = 32 summary(fit2) ## ## Call: ## lm(formula = mpg ~ disp + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Notice that the coefficient estimates are the same.\nTo make a term non-parametric, wrap the s function around the term (for splines; comes with the mgcv package). The gam package also has a lo function, for loess smoothing.\nfit3 \u0026lt;- gam(mpg ~ s(disp) + s(wt), data=mtcars) summary(fit3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp) + s(wt) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 20.0906 0.3429 58.59 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp) 6.263 7.386 6.373 0.000164 *** ## s(wt) 1.000 1.000 4.015 0.056434 . ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.896 Deviance explained = 92.1% ## GCV = 5.0715 Scale est. = 3.762 n = 32 Now, each predictor enters the model in a non-parametric, additive form. The nonparametric functions can be accessed by calling plot. For documentation, see ?plot.gam. Let‚Äôs plot the ‚Äúbivariate‚Äù scatterplots behind these curves too (these bivariate data actually use partial residuals).\nplot(fit3, residuals=TRUE) Looks like the ‚Äúweight‚Äù variable (wt) is quite linear. We can let it be linear, while the disp variable remains nonparametric. ‚ÄúWiggliness‚Äù of the smoothed fit can be controlled through the k argument of the s function, but this is chosen in a ‚Äúsmart‚Äù way by default.\nfit4 \u0026lt;- gam(mpg ~ s(disp, k=3) + wt, data=mtcars) summary(fit4) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp, k = 3) + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.1110 3.1336 9.928 1.1e-10 *** ## wt -3.4254 0.9649 -3.550 0.00138 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp) 1.93 1.995 9.724 0.000758 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.839 Deviance explained = 85.4% ## GCV = 6.659 Scale est. = 5.8413 n = 32 plot(fit4, residuals=TRUE) You can even combine predictors into a common smooth function:\nfit5 \u0026lt;- gam(mpg ~ s(disp, qsec) + wt, data=mtcars) summary(fit5) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp, qsec) + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 32.181 4.340 7.415 1.83e-07 *** ## wt -3.758 1.345 -2.794 0.0105 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp,qsec) 7.634 9.694 5.353 0.000312 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.903 Deviance explained = 93% ## GCV = 5.0335 Scale est. = 3.5181 n = 32 For each, the predict and residuals functions work in the same old way. Let‚Äôs use them to make a residual plot:\nqplot(predict(fit5), residuals(fit5)) + geom_abline(intercept=0, slope=0, linetype=\u0026quot;dashed\u0026quot;) + xlab(\u0026quot;Prediction (mean)\u0026quot;) + ylab(\u0026quot;Residuals\u0026quot;) For their documentation, see ?predict.gam and ?residuals.gam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"15bc441f2ba1534281e7873a7da1adae","permalink":"/post/gam_in_r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/gam_in_r/","section":"post","summary":"Generalized Additive Models To fit a GAM in R, we could use:\nthe function gam in the mgcv package, or the function gam in the gam package.  Differences between the two functions are discussed in the ‚ÄúDetails‚Äù section of the gam documentation in the mgcv package. Choose one, but don‚Äôt load both! mgcv tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that‚Äôs what is used in this tutorial.","tags":null,"title":"Generalized Additive Models","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"49f206e15c8f8776b088cdb872e95b94","permalink":"/project/igcop/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/igcop/","section":"project","summary":"Tools for accessing the IG and IGL family of copulas. R package.","tags":["R"],"title":"igcop","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5aae1ec86a01c71db8ff29652d4f8282","permalink":"/project/interpreting-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/interpreting-regression/","section":"project","summary":"An in-progress book on data-driven problem solving from a probabilistic lens. A modern take to statistics. Title may change.","tags":["Statistics"],"title":"Interpreting Regression","type":"project"},{"authors":null,"categories":null,"content":" JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.\nYou‚Äôll need to download and install JAGS. You can interact with JAGS through one of three R packages:\n runjags (recommended for this course)  Model written as a single string in R; possibly also allows you to input from file. Quick-start guide vignette: vignette(\"quickjags\", package=\"runjags\") Full user guide vignette: vignette(\"UserGuide\", package=\"runjags\")  rjags (sample code here)  Model read in from plain text file.  R2jags  this is dependent on R2winbugs, which I find doesn‚Äôt work well outside of Windows machines, so I‚Äôm more hesitant to use this package. Model written in R as a function, but using JAGS language; or inputted from file.   Also, the coda package is useful for working with the output of at least runjags.\nAbout the JAGS language:\n Generates samples of parameters based on the prior and statistical model. Need to specify which parameters you want to include in the output (aka ‚Äútrack‚Äù or ‚Äúmonitor‚Äù). Specify probability distributions similarly to R, except:  Draw samples using calls like dexp and dnorm, not rexp and rnorm. The JAGS version of rnorm uses the precision (=1/variance) instead of standard deviation. The documentation of JAGS code is not as nice as R. You have to look things up from a table-of-contents-style search from this document. Page 29 shows the aliases for various distributions.   For this week‚Äôs lab assignment (3), you‚Äôll only be using it to generate observations from a distribution. Let‚Äôs generate data from a N(0,2) distribution (that is, variance=2), and ignore the warning messages for this week.\nsuppressPackageStartupMessages(library(runjags)) my_model \u0026lt;- \u0026quot; model{ # This is a comment. theta ~ dnorm(0, 1/2) } \u0026quot; fit \u0026lt;- run.jags(my_model, monitor=\u0026quot;theta\u0026quot;, n.chains=1, sample=1000) ## Warning: No data was specified or found in the model file so the simulation was ## run withut data ## Calling the simulation... ## Welcome to JAGS 4.3.0 on Tue Sep 22 22:14:03 2020 ## JAGS is free software and comes with ABSOLUTELY NO WARRANTY ## Loading module: basemod: ok ## Loading module: bugs: ok ## . . Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 1 ## Total graph size: 5 ## . Initializing model ## . Adaptation skipped: model is not in adaptive mode. ## . Updating 4000 ## -------------------------------------------------| 4000 ## ************************************************** 100% ## . . Updating 1000 ## -------------------------------------------------| 1000 ## ************************************************** 100% ## . . . . Updating 0 ## . Deleting model ## . ## Note: the model did not require adaptation ## Simulation complete. Reading coda files... ## Coda files loaded successfully ## Calculating summary statistics... ## Warning: Convergence cannot be assessed with only 1 chain ## Finished running the simulation theta \u0026lt;- coda::as.mcmc(fit) head(theta) ## Markov Chain Monte Carlo (MCMC) output: ## Start = 5001 ## End = 5007 ## Thinning interval = 1 ## theta ## 5001 0.412988 ## 5002 -1.439260 ## 5003 0.520555 ## 5004 1.702830 ## 5005 0.840491 ## 5006 1.610820 ## 5007 1.567480 plot(theta) More sample code library(tidyverse) ## ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.0 ‚îÄ‚îÄ ## ‚úì ggplot2 3.3.2 ‚úì purrr 0.3.4 ## ‚úì tibble 3.0.3 ‚úì dplyr 1.0.2 ## ‚úì tidyr 1.1.2 ‚úì stringr 1.4.0 ## ‚úì readr 1.3.1 ‚úì forcats 0.5.0 ## ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ## x tidyr::extract() masks runjags::extract() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(runjags) n \u0026lt;- 50 dat \u0026lt;- tibble(x=rnorm(n), y=x + rnorm(n)) jagsdat \u0026lt;- c(as.list(dat), n=nrow(dat)) model \u0026lt;- \u0026quot;model{ for (i in 1:n) { y[i] ~ dnorm(beta*x[i], tau) } tau \u0026lt;- pow(sigma, -2) sigma ~ dunif(0, 100) beta ~ dnorm(0, 0.001) }\u0026quot; foo \u0026lt;- run.jags( model=model, monitor=c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;, \u0026quot;sigma\u0026quot;), data=jagsdat ) ## Warning: No initial value blocks found and n.chains not specified: 2 chains were ## used ## Warning: No initial values were provided - JAGS will use the same initial values ## for all chains ## Calling the simulation... ## Welcome to JAGS 4.3.0 on Tue Sep 22 22:14:07 2020 ## JAGS is free software and comes with ABSOLUTELY NO WARRANTY ## Loading module: basemod: ok ## Loading module: bugs: ok ## . . Reading data file data.txt ## . Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 2 ## Total graph size: 159 ## . Initializing model ## . Adapting 1000 ## -------------------------------------------------| 1000 ## ++++++++++++++++++++++++++++++++++++++++++++++++++ 100% ## Adaptation successful ## . Updating 4000 ## -------------------------------------------------| 4000 ## ************************************************** 100% ## . Failed to set trace monitor for beta0 ## Variable beta0 not found ## . Failed to set trace monitor for beta1 ## Variable beta1 not found ## . . Updating 10000 ## -------------------------------------------------| 10000 ## ************************************************** 100% ## . . . . . Updating 0 ## . Deleting model ## . ## Simulation complete. Reading coda files... ## Coda files loaded successfully ## Calculating summary statistics... ## Calculating the Gelman-Rubin statistic for 1 variables.... ## Finished running the simulation plot(coda::as.mcmc(foo)) ## Warning in as.mcmc.runjags(foo): Combining the 2 mcmc chains together  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e813129d9dd3d8ec5e743531ae949b23","permalink":"/post/jags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/jags/","section":"post","summary":"JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.\nYou‚Äôll need to download and install JAGS. You can interact with JAGS through one of three R packages:\n runjags (recommended for this course)  Model written as a single string in R; possibly also allows you to input from file. Quick-start guide vignette: vignette(\"","tags":null,"title":"JAGS Tutorial","type":"post"},{"authors":null,"categories":null,"content":" I aim to promote a probabilistic and trustworthy approach to data science. I do this by developing tools and content, teaching at UBC, and consulting.\nMy favourite application area is earth science and ecology. I love birds, invertebrates, and the polar regions.\nMotivation A probabilistic approach to data science means to embrace uncertainty. When we fail to recognize uncertainty, insights are drawn as though an omniscient expert is broadcasting an inevitability: national GDP will increase by 5% next year, or a river will peak to 0.5 meters below the town‚Äôs levee. Our understanding of each situation, in reality, is far less certain, and effectively communicating that uncertainty can make the difference when making big decisions such as whether to evacuate a town.\nConveying uncertainty requires the use of probability distributions. This means more than making an elusive Normal assumption, or fitting a test statistic to a t-distribution ‚Äì it means:\nBuilding realistic distributions.  This can be done using machine learning, simulation, or other techniques.  Interpreting these distributions in multiple ways, so that a more complete picture of the scenario can be given as opposed to providing a single value such as a mean.  This can be done using quantiles, plots, and relevant depictions of probability distributions.    Educational Leadership A major part of my role at UBC is in educational leadership. For me, educational leadership means connecting data scientists ‚Äì both aspiring and otherwise ‚Äì with appropriate statistical tools and methods for creating responsible data analyses. My contributions lie mostly in three domains:\n writing material to introduce statistical topics from a problem-first and probabilistic perspective; developing R packages to make probabilistic tools tangible; and consulting with and teaching students, colleagues, and organizations.   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e9e6b0f97a6aa3e949d37ece48e74541","permalink":"/mission/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/mission/","section":"","summary":"I aim to promote a probabilistic and trustworthy approach to data science. I do this by developing tools and content, teaching at UBC, and consulting.\nMy favourite application area is earth science and ecology. I love birds, invertebrates, and the polar regions.\nMotivation A probabilistic approach to data science means to embrace uncertainty. When we fail to recognize uncertainty, insights are drawn as though an omniscient expert is broadcasting an inevitability: national GDP will increase by 5% next year, or a river will peak to 0.","tags":null,"title":"Mission","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6483462f3258f30aa0b56689dbdc6fa0","permalink":"/project/rqdist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/rqdist/","section":"project","summary":"Build predictive distributions using linear quantile regression. R package.","tags":["R"],"title":"rqdist","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a7aea7ec29e633b4e5f072e6d1b4ddad","permalink":"/project/stat545/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/stat545/","section":"project","summary":"An open-access course on data wrangling, exploration, and analysis using the modern R landscape with git.","tags":["STAT 545","R"],"title":"STAT 545 @ UBC","type":"project"},{"authors":null,"categories":null,"content":" My philosophy on teaching can best be broken down into four categories: appropriate course structure, responsive teaching, motivation, and self-improvement.\nAppropriate Structure You and I don‚Äôt really know English. We only know enough for it to be a sufficiently useful tool for our own daily activities. I take this concept to heart as I design and deliver my courses. It‚Äôs fallacious to design a course around topics, but rather on building skills for a certain purpose. To me, these skills formulated as a list of learning objectives (LO‚Äôs) act as a ‚Äútrue north‚Äù that all components of a course point towards, and are the most important structure to the course. For example, course content on the topic ‚Äúheteroskedasticity‚Äù could be developed in many directions. The students, thinking they have to ‚Äúknow heteroskedasticity‚Äù, get stuck studying its never-ending scope. Instead, a learning objective such as ‚Äúmodify a linear model to appropriately incorporate heteroskedasticity based on data diagnostics‚Äù provides direction for the teaching team to create material, and clarity for the students. For concrete examples, take a look at the DSCI 551 notes, or my other course materials.\nClarity about the course structure and expectations is important for equipping students with a map of what they need to do to succeed. In addition to discussing how students should use the LO‚Äôs, describing the summative assessments and a course syllabus is also important, as well as making it easy to navigate course material (see, for example, the STAT 545A/547M syllabus website ‚Äì although the ‚ÄúLearning Outcomes‚Äù listed need updating from pre-2017). Informing the students of any changes or of any mistakes, no matter how painful, is also critical for maintaining clarity.\n Responsive teaching Sticking too close to course structure can stymie a course. This is because, while preparing course material, the instructor is not truly equipped with the clarity of how to most appropriately deliver material until the material has been delivered. For example, a question for the class might have seemed insightful when it was being prepared, but maybe during class the question doesn‚Äôt end up being insightful. Or, maybe the lecture delivery time was underestimated. The answer to addressing this issue is responsive teaching.\nResponsive teaching to me is about staying in touch with the class to get a feel for areas that need more or less attention. For example, it means: taking a break when the energy of the class is low; responding to class confusion by explaining something in a different way on the whiteboard; or spending more time on something after realizing its importance mid-class. In this way, teaching is more akin to improv comedy, where actors must respond to random cues from the audience (I‚Äôve taken lessons). I believe in embracing a mindset of letting go and trusting in yourself to respond appropriately the moment things go off course, and to also be humble and admit when you don‚Äôt know something.\nThis type of spontaneity sometimes also requires realizing and appropriately acting on your agency in adapting a course on the fly. Understanding that a course is flexible allows you to be nimble during contact hours with students, as opposed to feeling stuck in delivering the course exactly as it was originally laid out. However, it‚Äôs important to be strategic when making changes so that things remain orderly and as seamless as possible for the students, and to be clear about any changes made. I like using the start of class to check in and return to a roadmap of the course so that everyone remains on board a moving bus.\nContact time with students is critical for adapting teaching to the students, and provides tremendous value for students to actually enrol in a program as opposed to taking an online course. Perhaps the most effective method for doing this is making a point to engage with each student in lab. Equally as powerful are discussion-style office hours. I no longer hold my office hours in my office, because the office hour model whereby students just drop by to ask a question is just not effective. There ends up being a queue of students, usually asking common questions, and these students feel pressured to leave the office so that others can get a turn. Instead, I hold my office hours in a lab-like room suitable for collaboration. I end up leading a whole-group discussion prompted by student questions. This gives me even more insight as to how things are going in class, and allows me the opportunity to modify the course moving forward or make clarifications. Plus, the students express appreciation for this approach.\nSome other methods for connecting with the students are:\n being present on the course Slack channel (and sometimes even other course channels) to provide more insight, sending out a 1-minute long early survey about how the course is going, taking the time to talk to students who approach me during the mid-class break or after class, pausing to ask questions and check for insight in the classroom, applying active learning strategies such as think-pair-share or live coding, and checking in to see how things are going at the start of each class, by asking questions like ‚Äúhow are we feeling about the quiz coming up next week‚Äù, or ‚Äúhow are we feeling this week‚Äù.   Motivation Neither the presence of structure nor responsive teaching will suffice if the students aren‚Äôt excited or motivated about the course material, because the ultimately, learning happens outside the classroom, as the student engages with the material.\nTaking a problem-first approach helps with this. But it also requires some level of entertainment and enthusiasm from the instructor to keep students engaged. It means being aware of the lengthy (usually 80-minute) time frame that students are present for (and that‚Äôs just for one class), by taking a break mid-class, returning to the big-picture, and providing additional instructions for exercises for people who may have ‚Äúfallen off the bus‚Äù. It means telling stories. I‚Äôm pleased that I regularly receive ample praise on my instructor evaluations, and I encourage you to take a look.\n Self-Improvement What‚Äôs better than effective teaching? Teaching that continually becomes more effective.\nCommunity engagement is one method to become a better teacher. This means keeping an open dialogue and sharing experiences with other teachers, especially my colleagues. Aside from simple acts like sharing ideas and experiences through Slack and gatherings, I‚Äôm proud that my team gives and receives formative feedback on our teaching by visiting each other‚Äôs lectures. I also take the opportunity to partake in workshops, such as UBC‚Äôs Instructional Skills Workshop, and MDS‚Äôs annual workshop on Data Science education.\nAn after-action review is another effective method, involving capturing the insight you gain after teaching. There are three ways I engage in an after-action review. First, I capture regular insight throughout the delivery of the course as GitHub Issues, so that the insight can easily be referred to in the future and by any of my colleagues. Secondly, I find keeping a teaching journal that‚Äôs not tied to a specific course is useful for becoming a better teacher in general ‚Äì and it‚Äôs even easier now that my team has our lectures recorded. Thirdly, my colleagues and I engage in a ‚Äúretreat‚Äù at the end of each term, to discuss our insight on our courses and the MDS program as a whole.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb737296956c3f9cb1ea13f787e98de0","permalink":"/teaching/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"My philosophy on teaching can best be broken down into four categories: appropriate course structure, responsive teaching, motivation, and self-improvement.\nAppropriate Structure You and I don‚Äôt really know English. We only know enough for it to be a sufficiently useful tool for our own daily activities. I take this concept to heart as I design and deliver my courses. It‚Äôs fallacious to design a course around topics, but rather on building skills for a certain purpose.","tags":null,"title":"Teaching Philosophy","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"478d41a08bd0ce0385907859e59da585","permalink":"/project/youtube/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/youtube/","section":"project","summary":"Short videos promoting a clean and modern data analysis. Pairs with STAT 545.","tags":["STAT 545","R"],"title":"YouTube Channel","type":"project"}]