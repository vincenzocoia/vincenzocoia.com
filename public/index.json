[{"authors":["admin"],"categories":null,"content":"Hi!\nI\u0026rsquo;m an Assistant Professor of Teaching at the University of British Columbia, Vancouver. My priority here is to develop data science initiatives through the Department of Statistics.\nMy hobbies include outdoor recreation üå≤ such as hiking üëû, birding üê¶, skiing üéø, fishing üé£, canoeing üö£‚Äç‚ôÇÔ∏è, and others.\nSome of the things I do:\n I solve problems using statistics, machine learning, and data visualizations. I interact with students to make data science more approachable. I use R, git, and friends to help me do these things.  I can also identify birds, and other things about nature.\nHere are generic slides I use to introduce myself.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ed4d2245720b2d43a173bdec4161ed6","permalink":"/authors/vincenzo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vincenzo/","section":"authors","summary":"Hi!\nI\u0026rsquo;m an Assistant Professor of Teaching at the University of British Columbia, Vancouver. My priority here is to develop data science initiatives through the Department of Statistics.\nMy hobbies include outdoor recreation üå≤ such as hiking üëû, birding üê¶, skiing üéø, fishing üé£, canoeing üö£‚Äç‚ôÇÔ∏è, and others.\nSome of the things I do:\n I solve problems using statistics, machine learning, and data visualizations. I interact with students to make data science more approachable.","tags":null,"title":"Vincenzo Coia","type":"authors"},{"authors":null,"categories":null,"content":" This is an updated version of the original post from February 22, 2017.\nThis tutorial introduces contour plots, and how to plot them in ggplot2.\nWhat is a contour plot? Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?\nThe idea is to use contour lines, which are curves that indicate a constant height.\nImagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude. Then trace (on your map) the shapes formed by the new (flat) mountain tops. These curves are contour lines. Choose a differential such as 50 meters, and draw these curves for altitudes ‚Ä¶800m, 850m, 900m, 950m, 1000m, ‚Ä¶ ‚Äì the result is a contour plot (or topographic map, if it‚Äôs a map).\nIn general, contour plots are useful for functions of two variables (like a bivariate gaussian density).\nWe‚Äôll look at examples in the next section.\nNotes on contours:\n They never cross. The steepest slope at a point is parallel to the contour line. They aren‚Äôt entirely ambiguous. For example, you can‚Äôt tell whether or not the mountains are actually mountains, or whether they‚Äôre holes/valleys! Sometimes you can add colour to indicate depth; other times (like in topographic maps) you can indicate elevation directly as numbers beside contour lines. Other times, this is not required, because the context makes it obvious.  There are two ways you can make contour plots in ggplot2 ‚Äì but they‚Äôre both for quite different purposes. Let‚Äôs load ggplot2 through the tidyverse:\nlibrary(tidyverse) theme_set(theme_minimal())  Method 1: Approximate a bivariate density: geom_density2d() This method approximates a bivariate density from data.\nFirst, recall how this is done in the univariate case. A little kernel function (like a shrunken bell curve) is placed over each data point, and these are added together to get a density estimate:\nset.seed(373) x \u0026lt;- rnorm(1000) tibble(x = x) %\u0026gt;% ggplot(aes(x)) + geom_density() We can do the same thing to get a bivariate density, except with little bivariate kernel functions (like shrunken bivariate Gaussian densities). But, we can‚Äôt just simply put ‚Äúdensity height‚Äù on the vertical axis ‚Äì we need that for the second dimension. Instead, we can use contour plots.\nThis is the contour plot that ggplot2‚Äôs geom_density2d() does: builds a bivariate kernel density estimate (based on data), then makes a contour plot out of it:\ny \u0026lt;- rnorm(1000) tibble(x = x, y = y) %\u0026gt;% ggplot(aes(x, y)) + geom_density2d() + theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) Based on context (this is a density), we know that this is a ‚Äúhill‚Äù and not a ‚Äúhole‚Äù. If you were to start at some point at the ‚Äúbottom‚Äù of the hill, the steepest way up would be perpendicular to the contours. The highest point on the hill is within the middle-most circle.\n Method 2: General Contour Plots: geom_contour() You can also make contour plots that aren‚Äôt a kernel density estimate (necessarily), using geom_contour(). This is based off of any bivariate function.\nBasics Suppose we want to make a contour plot of the bivariate function \\[f(x, y) = x ^ 2 + \\sin(y)\\] over the rectangle \\(-2\u0026lt;x\u0026lt;2\\) and \\(-5\u0026lt;y\u0026lt;5\\).\nMake a grid over the rectangle. It must be a grid ‚Äì geom_contour() won‚Äôt work otherwise. The expand_grid() function is handy for this, filling in all combinations of its input.  f \u0026lt;- function(x, y) x ^ 2 + sin(y) x \u0026lt;- seq(-2, 2, length.out = 100) y \u0026lt;- seq(-5, 5, length.out = 100) (dat \u0026lt;- expand_grid(x = x, y = y)) ## # A tibble: 10,000 x 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -2 -5 ## 2 -2 -4.90 ## 3 -2 -4.80 ## 4 -2 -4.70 ## 5 -2 -4.60 ## 6 -2 -4.49 ## 7 -2 -4.39 ## 8 -2 -4.29 ## 9 -2 -4.19 ## 10 -2 -4.09 ## # ‚Ä¶ with 9,990 more rows Evaluate the function at each of the grid points. Make sure you end up with a data frame with three columns: two for the x and y coordinates, and one for the evaluated function (called z below).  (dat \u0026lt;- mutate(dat, z = f(x, y))) ## # A tibble: 10,000 x 3 ## x y z ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -2 -5 4.96 ## 2 -2 -4.90 4.98 ## 3 -2 -4.80 5.00 ## 4 -2 -4.70 5.00 ## 5 -2 -4.60 4.99 ## 6 -2 -4.49 4.98 ## 7 -2 -4.39 4.95 ## 8 -2 -4.29 4.91 ## 9 -2 -4.19 4.87 ## 10 -2 -4.09 4.81 ## # ‚Ä¶ with 9,990 more rows Use this data frame with ggplot2‚Äôs geom_contour(). The x and y aesthetics should be the two grid columns, and the z aesthetic should be mapped to the column with the evaluated function (z here).  ggplot(dat, aes(x, y)) + geom_contour(aes(z = z)) + theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) Note that finer grids yield plots with higher accuracy. Here‚Äôs an example of a rough grid, whose contours are jagged:\nx \u0026lt;- seq(-2, 2, length.out = 10) y \u0026lt;- seq(-5, 5, length.out = 10) expand_grid(x = x, y = y) %\u0026gt;% mutate(z = f(x, y)) %\u0026gt;% ggplot(aes(x, y)) + geom_contour(aes(z = z)) + theme(axis.title.y = element_text(angle = 0, vjust = 0.5))  Example using the volcano data R comes with a dataset containing the altitudes of a volcano, Maunga Whau (Mt Eden), stored in the datasets variable volcano. You‚Äôll notice that the dataset is literally a grid of \\(87 \\times 61\\) altitudes ‚Äì here are the first six rows and columns:\nvolcano[1:6, 1:6] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 100 100 101 101 101 101 ## [2,] 101 101 102 102 102 102 ## [3,] 102 102 103 103 103 103 ## [4,] 103 103 104 104 104 104 ## [5,] 104 104 105 105 105 105 ## [6,] 105 105 105 106 106 106 If you‚Äôd like, you can take a look at a 3D rendering of the volcano using the rgl package‚Äôs surface3d() function. The code for doing this is directly in the documentation of the surface3d() function.\nIn order to make a contour plot with ggplot2‚Äôs geom_contour(), we‚Äôll first need to turn this into a tidy data frame with three columns. You could use as.vector(volcano) to get a vector of altitudes, and line that up with a grid laid out as two columns, but I‚Äôm not going to take any chances here, so I‚Äôll opt to use pivot_longer(). We don‚Äôt know much about the latitude and longitude, so their values are arbitrary.\n(volcano_tidy \u0026lt;- as_tibble(volcano, .name_repair = \u0026quot;universal\u0026quot;) %\u0026gt;% mutate(latitude = 1:n()) %\u0026gt;% pivot_longer(!latitude, names_to = \u0026quot;longitude\u0026quot;, values_to = \u0026quot;altitude\u0026quot;)) ## # A tibble: 5,307 x 3 ## latitude longitude altitude ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 ...1 100 ## 2 1 ...2 100 ## 3 1 ...3 101 ## 4 1 ...4 101 ## 5 1 ...5 101 ## 6 1 ...6 101 ## 7 1 ...7 101 ## 8 1 ...8 100 ## 9 1 ...9 100 ## 10 1 ...10 100 ## # ‚Ä¶ with 5,297 more rows Fix up the longitude (taking the negative to reverse the order, because data are provided as east to west):\n(volcano_tidy \u0026lt;- volcano_tidy %\u0026gt;% mutate(longitude = longitude %\u0026gt;% str_remove(\u0026quot;[\\\\.]{3}\u0026quot;) %\u0026gt;% as.numeric() %\u0026gt;% `-`())) ## # A tibble: 5,307 x 3 ## latitude longitude altitude ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -1 100 ## 2 1 -2 100 ## 3 1 -3 101 ## 4 1 -4 101 ## 5 1 -5 101 ## 6 1 -6 101 ## 7 1 -7 101 ## 8 1 -8 100 ## 9 1 -9 100 ## 10 1 -10 100 ## # ‚Ä¶ with 5,297 more rows Now the contour plot comes for free:\nggplot(volcano_tidy, aes(longitude, latitude)) + geom_contour(aes(z = altitude)) + theme_void() But, you can‚Äôt tell that the inner circles actually represent a hole (a caldera), not a peak. Let‚Äôs add colour by mapping the ..height.. ‚Äúvariable‚Äù to the colour aesthetic of geom_contour(). This will also create a legend for altitude.\nggplot(volcano_tidy, aes(longitude, latitude)) + geom_contour(aes(z = altitude, colour = ..level..)) + theme_void() + scale_color_continuous(\u0026quot;Altitude\u0026quot;) We can somewhat tell that the highest point is within the lightest blue area, just below the caldera. But, we would get a better sense of the terrain by adding more contour lines. You can use the bins argument in the geom_contour() function to indicate the number of altitudes for which to draw contours, or binwidth to specify a range of altitudes. Let‚Äôs make 20 contours with bins = 20:\nggplot(volcano_tidy, aes(longitude, latitude)) + geom_contour(aes(z = altitude, colour = ..level..), bins = 20) + theme_void() + scale_color_continuous(\u0026quot;Altitude\u0026quot;)   ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"99b568f8520e1ae1a5032282960fc4f7","permalink":"/post/contour_plots/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/contour_plots/","section":"post","summary":"This is an updated version of the original post from February 22, 2017.\nThis tutorial introduces contour plots, and how to plot them in ggplot2.\nWhat is a contour plot? Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?\nThe idea is to use contour lines, which are curves that indicate a constant height.","tags":null,"title":"Contour Plots with ggplot2","type":"post"},{"authors":null,"categories":null,"content":" It‚Äôs important to me that I promote an inclusive environment both in and outside of the classroom, both in person and online. I will not tolerate discriminatory behaviour.\nTo promote an inclusive environment, I like to challenge myself by asking, ‚Äúwould I respond differently if this person was a different gender? race? looked a different way?‚Äù ‚Äì this helps me identify my own implicit biases. Despite my efforts, if I ever do discriminate against someone, or make an uncalled for assumption, I welcome being called out on it, and I sincerely apologize and would like to right the mistake.\n","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"1ab071aa4c9f530acf96cd48e859ecea","permalink":"/approach/inclusion/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/approach/inclusion/","section":"approach","summary":"It‚Äôs important to me that I promote an inclusive environment both in and outside of the classroom, both in person and online. I will not tolerate discriminatory behaviour.\nTo promote an inclusive environment, I like to challenge myself by asking, ‚Äúwould I respond differently if this person was a different gender? race? looked a different way?‚Äù ‚Äì this helps me identify my own implicit biases. Despite my efforts, if I ever do discriminate against someone, or make an uncalled for assumption, I welcome being called out on it, and I sincerely apologize and would like to right the mistake.","tags":null,"title":"Inclusivity","type":"approach"},{"authors":null,"categories":null,"content":"  I aim to promote a probabilistic and trustworthy approach to data science. I do this by developing tools and content, teaching at UBC, and consulting.\nMy favourite application area is hydrology, geomorphology, and ecology. I love invertebrates, oceans, and the polar regions.\nA Probabilistic Approach to Data Science A probabilistic approach to data science means embracing and communicating uncertainty. When we fail to recognize uncertainty, insights are drawn as though an omniscient expert is broadcasting an inevitability: national GDP will increase by 5% next year, or a river will peak to 0.5 meters below the town‚Äôs levee. Our understanding of each situation, in reality, is far less certain, and effectively communicating that uncertainty can make the difference when making big decisions such as whether to evacuate a town.\nConveying uncertainty requires the use of probability distributions. This means more than making an elusive Normal assumption, or fitting a test statistic to a t-distribution ‚Äì it means:\nBuilding realistic data-driven distributions Interpreting these distributions in ways that paint a more complete picture of the scenario.   Educational Leadership A major part of my role at UBC is in educational leadership. For me, educational leadership means connecting data scientists ‚Äì both aspiring and otherwise ‚Äì with appropriate statistical tools and methods for creating responsible data analyses. My contributions lie mostly in three domains:\n writing material to introduce statistical topics from a problem-first and probabilistic perspective; developing R packages to make probabilistic tools approachable; and consulting with and teaching students, colleagues, and organizations.   ","date":1613865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613865600,"objectID":"5c9e6d275861e0e1f30fad0bb1169099","permalink":"/approach/mission/","publishdate":"2021-02-21T00:00:00Z","relpermalink":"/approach/mission/","section":"approach","summary":"I aim to promote a probabilistic and trustworthy approach to data science. I do this by developing tools and content, teaching at UBC, and consulting.\nMy favourite application area is hydrology, geomorphology, and ecology. I love invertebrates, oceans, and the polar regions.\nA Probabilistic Approach to Data Science A probabilistic approach to data science means embracing and communicating uncertainty. When we fail to recognize uncertainty, insights are drawn as though an omniscient expert is broadcasting an inevitability: national GDP will increase by 5% next year, or a river will peak to 0.","tags":null,"title":"Mission","type":"approach"},{"authors":null,"categories":null,"content":"","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600128000,"objectID":"318047a5530e3b174b022ec0740f409c","permalink":"/post/communicating_data/","publishdate":"2020-09-15T00:00:00Z","relpermalink":"/post/communicating_data/","section":"post","summary":"Some best practices for communicate data through visuals. Inspired by Claus Wilke's [\"Fundamentals of Data Visualization\"](https://clauswilke.com/dataviz).","tags":null,"title":"Communicating Data","type":"post"},{"authors":null,"categories":null,"content":"  JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.\nYou‚Äôll need to download and install JAGS. You can interact with JAGS through one of three R packages:\n runjags (recommended for this course)  Model written as a single string in R; possibly also allows you to input from file. Quick-start guide vignette: vignette(\"quickjags\", package=\"runjags\") Full user guide vignette: vignette(\"UserGuide\", package=\"runjags\")  rjags (sample code here)  Model read in from plain text file.  R2jags  this is dependent on R2winbugs, which I find doesn‚Äôt work well outside of Windows machines, so I‚Äôm more hesitant to use this package. Model written in R as a function, but using JAGS language; or inputted from file.   Also, the coda package is useful for working with the output of at least runjags.\nAbout the JAGS language:\n Generates samples of parameters based on the prior and statistical model. Need to specify which parameters you want to include in the output (aka ‚Äútrack‚Äù or ‚Äúmonitor‚Äù). Specify probability distributions similarly to R, except:  Draw samples using calls like dexp and dnorm, not rexp and rnorm. The JAGS version of rnorm uses the precision (=1/variance) instead of standard deviation. The documentation of JAGS code is not as nice as R. You have to look things up from a table-of-contents-style search from this document. Page 29 shows the aliases for various distributions.   For this week‚Äôs lab assignment (3), you‚Äôll only be using it to generate observations from a distribution. Let‚Äôs generate data from a N(0,2) distribution (that is, variance=2), and ignore the warning messages for this week.\nlibrary(runjags) my_model \u0026lt;- \u0026quot; model{ # This is a comment. theta ~ dnorm(0, 1/2) } \u0026quot; fit \u0026lt;- run.jags(my_model, monitor = \u0026quot;theta\u0026quot;, n.chains = 1, sample = 1000) ## Calling the simulation... ## Welcome to JAGS 4.3.0 on Thu Sep 30 22:16:26 2021 ## JAGS is free software and comes with ABSOLUTELY NO WARRANTY ## Loading module: basemod: ok ## Loading module: bugs: ok ## . . Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 1 ## Total graph size: 5 ## . Initializing model ## . Adaptation skipped: model is not in adaptive mode. ## . Updating 4000 ## -------------------------------------------------| 4000 ## ************************************************** 100% ## . . Updating 1000 ## -------------------------------------------------| 1000 ## ************************************************** 100% ## . . . . Updating 0 ## . Deleting model ## . ## Note: the model did not require adaptation ## Simulation complete. Reading coda files... ## Coda files loaded successfully ## Calculating summary statistics... ## Finished running the simulation theta \u0026lt;- coda::as.mcmc(fit) head(theta) ## Markov Chain Monte Carlo (MCMC) output: ## Start = 5001 ## End = 5007 ## Thinning interval = 1 ## theta ## 5001 1.2975400 ## 5002 0.7812320 ## 5003 0.7415140 ## 5004 2.0970800 ## 5005 -0.0261437 ## 5006 -2.1801300 ## 5007 -0.9571290 plot(theta) More sample code library(tidyverse) ## ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.0 ‚îÄ‚îÄ ## ‚úì ggplot2 3.3.2 ‚úì purrr 0.3.4 ## ‚úì tibble 3.0.3 ‚úì dplyr 1.0.2 ## ‚úì tidyr 1.1.2 ‚úì stringr 1.4.0 ## ‚úì readr 1.3.1 ‚úì forcats 0.5.0 ## ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ## x tidyr::extract() masks runjags::extract() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(runjags) n \u0026lt;- 50 dat \u0026lt;- tibble(x = rnorm(n), y = x + rnorm(n)) jagsdat \u0026lt;- c(as.list(dat), n = nrow(dat)) model \u0026lt;- \u0026quot;model{ for (i in 1:n) { y[i] ~ dnorm(beta*x[i], tau) } tau \u0026lt;- pow(sigma, -2) sigma ~ dunif(0, 100) beta ~ dnorm(0, 0.001) }\u0026quot; foo \u0026lt;- run.jags( model = model, monitor = c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;, \u0026quot;sigma\u0026quot;), data = jagsdat ) ## Warning: No initial value blocks found and n.chains not specified: 2 chains were ## used ## Warning: No initial values were provided - JAGS will use the same initial values ## for all chains ## Calling the simulation... ## Welcome to JAGS 4.3.0 on Thu Sep 30 22:16:29 2021 ## JAGS is free software and comes with ABSOLUTELY NO WARRANTY ## Loading module: basemod: ok ## Loading module: bugs: ok ## . . Reading data file data.txt ## . Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 2 ## Total graph size: 159 ## . Initializing model ## . Adapting 1000 ## -------------------------------------------------| 1000 ## ++++++++++++++++++++++++++++++++++++++++++++++++++ 100% ## Adaptation successful ## . Updating 4000 ## -------------------------------------------------| 4000 ## ************************************************** 100% ## . Failed to set trace monitor for beta0 ## Variable beta0 not found ## . Failed to set trace monitor for beta1 ## Variable beta1 not found ## . . Updating 10000 ## -------------------------------------------------| 10000 ## ************************************************** 100% ## . . . . . Updating 0 ## . Deleting model ## . ## Simulation complete. Reading coda files... ## Coda files loaded successfully ## Calculating summary statistics... ## Calculating the Gelman-Rubin statistic for 1 variables.... ## Finished running the simulation plot(coda::as.mcmc(foo)) ## Warning in as.mcmc.runjags(foo): Combining the 2 mcmc chains together  ","date":1520035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520035200,"objectID":"69ac5c6f48ec92e2913a16d225600370","permalink":"/post/jags/","publishdate":"2018-03-03T00:00:00Z","relpermalink":"/post/jags/","section":"post","summary":"JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.\nYou‚Äôll need to download and install JAGS. You can interact with JAGS through one of three R packages:\n runjags (recommended for this course)  Model written as a single string in R; possibly also allows you to input from file. Quick-start guide vignette: vignette(\"","tags":null,"title":"JAGS Tutorial","type":"post"},{"authors":null,"categories":null,"content":"You all know the drill \u0026ndash; you\u0026rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?\nRegression trees, loess, linear regression\u0026hellip; you name it, they\u0026rsquo;re all in pursuit of the mean (well, almost all). But the true outcome is random. It has a distribution. Are you sure you want the mean of that distribution?\nYou might say \u0026ldquo;Yes! It ensures my prediction is as close as possible to the outcome!\u0026rdquo; If this is indeed what you want, the mean still might not be your best choice \u0026ndash; it only ensures the mean squared error is minimized.\nThere are a suite of other options that might be more appropriate than the mean. The good thing is, your favourite supervised learning method probably has a natural extension for estimating these alternatives. Let\u0026rsquo;s investigate the quantities you might care about.\nThe Median No, the median isn\u0026rsquo;t just an inferior version of the mean, to be used under the unfortunate presence of outliers.\nIf I randomly pick a data scientist, what do you think their salary would be? This distribution has a right-skew, so chances are, your data scientist earns less than the mean. Predict the median, and you\u0026rsquo;ll have a 50% chance that your data scientist does earn at least what you predict.\nIn short, use the median when you want your prediction to be exceeded with a coin toss.\nMinimize the mean absolute error to get this prediction.\nHigher (or lower) Quantiles Want to make it to an interview on time? You add some \u0026ldquo;buffer time\u0026rdquo; to the expected travel time, right? What you\u0026rsquo;re after is a high quantile of travel time \u0026ndash; something like the 0.99-quantile, so that there is only a small chance you\u0026rsquo;ll be late (1% in this case).\nUse a high (or low) quantile if you want a conservative (or liberal) prediction \u0026ndash; or both, if you want a prediction interval.\nMinimize the mean rho function to get this prediction.\nThe Mean The mean is useful when we care about totals. Want to know how much gas a vehicle uses? You\u0026rsquo;re after the mean, because the total quantity drawn out over time is what matters.\nMinimize the mean squared error to get this prediction.\nOther Options Do you really need to distill your prediction down to a single number? Consider looking at the entire distribution of the outcome as your prediction (typically conditional on predictors) \u0026ndash; after all, this conveys the entire uncertainty about the outcome. This is known as probabilistic forecasting.\nThere are other measures, too. Expected shortfall is useful for risk analysis, or even expectiles. Maybe you care about variance or skewness for some reason. Whatever you want to get at, just make sure you ask yourself what you actually care about. You have an entire distribution to distill!\n(Photo from Pexels)\n","date":1518912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518912000,"objectID":"20cc807a1ae074bf3ee732410f15765b","permalink":"/post/missing_question/","publishdate":"2018-02-18T00:00:00Z","relpermalink":"/post/missing_question/","section":"post","summary":"You all know the drill \u0026ndash; you\u0026rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?\nRegression trees, loess, linear regression\u0026hellip; you name it, they\u0026rsquo;re all in pursuit of the mean (well, almost all). But the true outcome is random.","tags":["supervised learning","mean","quantile","median","probabilistic forecasting"],"title":"The missing question in supervised learning","type":"post"},{"authors":null,"categories":null,"content":"  Generalized Additive Models To fit a GAM in R, we could use:\nthe function gam in the mgcv package, or the function gam in the gam package.  Differences between the two functions are discussed in the ‚ÄúDetails‚Äù section of the gam documentation in the mgcv package. Choose one, but don‚Äôt load both! mgcv tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that‚Äôs what is used in this tutorial. But the gam package has similar workings.\nThe gam function works similarly to other regression functions, but the formula specification is different. Let‚Äôs go through different formula specifications, doing regression on the mtcars dataset in R.\nThe formula mpg ~ disp + wt gives you a linear model. It indicates that disp and wt both enter the model in a linear fashion.\nlibrary(tidyverse) library(broom) library(mgcv) fit1 \u0026lt;- gam(mpg ~ disp + wt, data = mtcars) fit2 \u0026lt;- lm(mpg ~ disp + wt, data = mtcars) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ disp + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## ## R-sq.(adj) = 0.766 Deviance explained = 78.1% ## GCV = 9.3863 Scale est. = 8.5063 n = 32 summary(fit2) ## ## Call: ## lm(formula = mpg ~ disp + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Notice that the coefficient estimates are the same.\nTo make a term non-parametric, wrap the s() function around the term (for splines; comes with the mgcv package). The gam package also has a lo function, for loess smoothing.\nfit3 \u0026lt;- gam(mpg ~ s(disp) + s(wt), data = mtcars) summary(fit3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp) + s(wt) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 20.0906 0.3429 58.59 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp) 6.263 7.386 6.373 0.000164 *** ## s(wt) 1.000 1.000 4.015 0.056434 . ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.896 Deviance explained = 92.1% ## GCV = 5.0715 Scale est. = 3.762 n = 32 Now, each predictor enters the model in a non-parametric, additive form. The nonparametric functions can be accessed by calling plot(). For documentation, see ?plot.gam. Let‚Äôs plot the ‚Äúbivariate‚Äù scatterplots behind these curves too (these bivariate data actually use partial residuals).\nplot(fit3, residuals = TRUE) Looks like the ‚Äúweight‚Äù variable (wt) is quite linear. We can let it be linear, while the disp variable remains nonparametric. ‚ÄúWiggliness‚Äù of the smoothed fit can be controlled through the k argument of the s() function, but this is chosen in a ‚Äúsmart‚Äù way by default.\nfit4 \u0026lt;- gam(mpg ~ s(disp, k = 3) + wt, data = mtcars) summary(fit4) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp, k = 3) + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.1110 3.1336 9.928 1.1e-10 *** ## wt -3.4254 0.9649 -3.550 0.00138 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp) 1.93 1.995 9.724 0.000758 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.839 Deviance explained = 85.4% ## GCV = 6.659 Scale est. = 5.8413 n = 32 plot(fit4, residuals = TRUE) You can even combine predictors into a common smooth function:\nfit5 \u0026lt;- gam(mpg ~ s(disp, qsec) + wt, data = mtcars) summary(fit5) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp, qsec) + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 32.181 4.340 7.415 1.83e-07 *** ## wt -3.758 1.345 -2.794 0.0105 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp,qsec) 7.634 9.694 5.353 0.000312 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.903 Deviance explained = 93% ## GCV = 5.0335 Scale est. = 3.5181 n = 32 For each, the predict() and residuals() functions work in the same old way, but let‚Äôs use the augment() function from the broom package instead, and make a residual plot:\naugment(fit5, newdata = mtcars) %\u0026gt;% mutate(Residuals = mpg - .fitted) %\u0026gt;% ggplot(aes(.fitted, Residuals)) + geom_point() + geom_abline(intercept = 0, slope = 0, linetype = \u0026quot;dashed\u0026quot;) + xlab(\u0026quot;Prediction (mean)\u0026quot;) + theme_minimal() For their documentation, see ?predict.gam and ?residuals.gam.\n ","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"143356193646b28198c2b53ec84f8c35","permalink":"/post/gam/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/post/gam/","section":"post","summary":"Generalized Additive Models To fit a GAM in R, we could use:\nthe function gam in the mgcv package, or the function gam in the gam package.  Differences between the two functions are discussed in the ‚ÄúDetails‚Äù section of the gam documentation in the mgcv package. Choose one, but don‚Äôt load both! mgcv tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that‚Äôs what is used in this tutorial.","tags":null,"title":"Generalized Additive Models","type":"post"},{"authors":null,"categories":null,"content":"  UPDATE: For a clean implementation of mixture distributions, check out the distplyr R package.\nThis tutorial introduces the concept of a mixture distribution. We‚Äôll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.\nIntuition Let‚Äôs start by looking at a basic experiment:\nFlip a coin. If the outcome is heads, generate a N(0,1) random variable. If the outcome is tails, generate a N(4,1) random variable. We‚Äôll let \\(X\\) denote the final result.  \\(X\\) is a random variable with some distribution (spoiler: it‚Äôs a mixture distribution). Let‚Äôs perform the experiment 1000 times to get 1000 realizations of \\(X\\), and make a histogram to get a sense of the distribution \\(X\\) follows. To make sure the histogram represents an estimate of the density, we‚Äôll make sure the area of the bars add to 1 (with the ..density.. option).\nlibrary(tidyverse) set.seed(44) X \u0026lt;- numeric(0) coin \u0026lt;- integer(0) for (i in 1:1000) { coin[i] \u0026lt;- rbinom(1, size = 1, prob = 0.5) # flip a coin. 0=heads, 1=tails. if (coin[i] == 0) { # heads X[i] \u0026lt;- rnorm(1, mean = 0, sd = 1) } else { # tails X[i] \u0026lt;- rnorm(1, mean = 4, sd = 1) } } (p \u0026lt;- qplot(X, ..density.., geom = \u0026quot;histogram\u0026quot;, bins = 30)) Let‚Äôs try to reason our way to figuring out the overall density. Keep in mind that this density (like all densities) is one curve. We‚Äôll say we‚Äôve succeeded at finding the density if our density is close to the histogram.\nIt looks like the histogram is made up of two normal distributions ‚Äúsuperimposed‚Äù. These ought to be related to the N(0,1) and N(4,1) distributions, so to start, let‚Äôs plot these two Gaussian densities overtop of the histogram.\ntibble(X = X) %\u0026gt;% ggplot(aes(X)) + geom_histogram(aes(y = ..density..), bins = 30) + stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1), mapping = aes(colour = \u0026quot;Heads\u0026quot;)) + stat_function(fun = function(x) dnorm(x, mean = 4, sd = 1), mapping = aes(colour = \u0026quot;Tails\u0026quot;)) + scale_color_discrete(\u0026quot;Coin Flip\u0026quot;) Well, the two Gaussian distributions are in the correct location, and it even looks like they have the correct spread, but they‚Äôre too tall.\nSomething to note at this point: the two curves plotted above are separate (component) distributions. We‚Äôre trying to figure out the distribution of \\(X\\) ‚Äì which, again, is a single curve, and is estimated by the histogram. At this point, we only suspect that the distribution of \\(X\\) is some combination of these two Gaussian distributions.\nSo, why are the Gaussian curves too tall? Because each one represents the distribution if we only ever flip either heads or tails (for example, the red distribution happens when we only ever flip heads). But since we flip heads half of the time, and tails half of the time, these probabilities (more accurately, densities) ought to be reduced by half. Let‚Äôs add these ‚Äúsemi‚Äù component distributions to the plot:\n(p \u0026lt;- tibble(X = X) %\u0026gt;% ggplot(aes(X)) + geom_histogram(aes(y = ..density..), bins = 30) + stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1)*0.5, mapping = aes(colour = \u0026quot;Heads\u0026quot;, linetype = \u0026quot;Semi\u0026quot;)) + stat_function(fun = function(x) dnorm(x, mean = 4, sd = 1)*0.5, mapping = aes(colour = \u0026quot;Tails\u0026quot;, linetype = \u0026quot;Semi\u0026quot;)) + stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1), mapping = aes(colour = \u0026quot;Heads\u0026quot;, linetype = \u0026quot;Full\u0026quot;)) + stat_function(fun = function(x) dnorm(x, mean = 4, sd = 1), mapping = aes(colour = \u0026quot;Tails\u0026quot;, linetype = \u0026quot;Full\u0026quot;)) + scale_color_discrete(\u0026quot;Coin Flip\u0026quot;) + scale_linetype_discrete(\u0026quot;Distribution\u0026quot;)) Looks like they line up quite nicely!\nBut these two curves are still separate ‚Äì we need one overall curve if we are to find the distribution of \\(X\\). So we need to combine them somehow. It might look at first that we can just take the upper-most of the ‚Äòsemi‚Äô curves (i.e., the maximum of the two), but looking in between the two curves reveals that the histogram is actually larger than either curve here. It turns out that the two ‚Äòsemi‚Äô curves are added to get the final curve:\np + stat_function(fun = function(x) dnorm(x, mean = 0, sd = 1) * 0.5 + dnorm(x, mean = 4, sd = 1) * 0.5, mapping = aes(linetype = \u0026quot;Full\u0026quot;)) The intuition behind adding the densities is that an outcome for \\(X\\) comes from both components, so both contribute some density.\nEven though the random variable \\(X\\) is made up of two components, at the end of the day, it‚Äôs still overall just a random variable with some density. And like all densities, the density of \\(X\\) is just one curve. But, this density happens to be made up of the components, as we‚Äôll see next.\n General Scenario The two normal distributions from above are called component distributions. In general, we can have any number of these (not just two) to make a mixture distribution. And, instead of selecting the component distribution with coin tosses, they‚Äôre chosen according to some generic probabilities called the mixture probabilities.\nIn general, here‚Äôs how we make a mixture distribution with \\(K\\) component Gaussian distributions with densities \\(\\phi_1(x), \\ldots, \\phi_K(x)\\):\nChoose one of the \\(K\\) components, randomly, with mixture probabilities \\(\\pi_1, \\ldots, \\pi_K\\) (which, by necessity, add to 1). Generate a random variable from the selected component distribution. Call the result \\(X\\).  Note: we can use more than just Gaussian component distributions! But this tutorial won‚Äôt demonstrate that.\nThat‚Äôs how we generate a random variable with a mixture distribution, but what‚Äôs its density? We can derive that by the law of total probability. Let \\(C\\) be the selected component number; then the component distributions are actually the distribution of \\(X\\) conditional on the component number. We get: \\[ f_X\\left(x\\right) = \\sum_{k=1}^{K} f_{X|C}\\left(x \\mid c\\right) P\\left(C=c\\right) = \\sum_{k=1}^{K} \\phi_k\\left(x\\right) \\pi_k. \\]\nNotes:  The intuition described in the previous section matches up with this result. For \\(K=2\\) components determined by a coin toss \\((\\pi_1=\\pi_2=0.5),\\) we have \\[ f_X\\left(x\\right) = \\phi\\left(x\\right)0.5 + \\phi\\left(x-4\\right)0.5, \\] which is the black curve in the previous plot. This tutorial works with univariate data. But mixture distributions can be multivariate, too. A \\(d\\)-variate mixture distribution can be made by replacing the component distributions with \\(d\\)-variate distributions. Just be sure to distinguish between the dimension of the data \\(d\\) and the number of components \\(K\\). We could just describe a mixture distribution by its density, just like we can describe a normal distribution by its density. But, describing mixture distributions by its component distributions together with the mixture probabilities, we obtain an excellent interpretation of the mixture distribution. This interpretation is (it‚Äôs also called a data generating process): (1) randomly choose a component, and (2) generate from that component. This interpretation is useful for cluster analysis, because the data clusters can be thought of as being generated by the component distributions, and the proportion of data in each cluster is determined by the mixture probabilities.    Learning Points  A mixture distribution can be described by its mixing probabilities \\(\\pi_1, \\ldots, \\pi_K\\) and component distributions \\(\\phi_1(x), \\ldots, \\phi_K(x)\\). A mixture distribution can also be described by a single density (like all continuous random variables).  This density is a single curve if data are univariate; a single ‚Äúsurface‚Äù if the data are bivariate; and higher dimensional surfaces if the data are higher dimensional.  To get the density from the mixing probabilities and component distributions, we can use the formula indicated in the above section (based on the law of total probability).   ","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"011e7a31497ff7779c67cdff23d2d924","permalink":"/post/mixture_distributions/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/mixture_distributions/","section":"post","summary":"UPDATE: For a clean implementation of mixture distributions, check out the distplyr R package.\nThis tutorial introduces the concept of a mixture distribution. We‚Äôll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.\nIntuition Let‚Äôs start by looking at a basic experiment:\nFlip a coin. If the outcome is heads, generate a N(0,1) random variable.","tags":null,"title":"Mixture distributions","type":"post"},{"authors":null,"categories":null,"content":"\u0026ldquo;Probabilistic Thinking in Data Science\u0026rdquo; Initiative Resources to migrate to this new resource.\nVision for how people will engage:\n As if they are learning something for a course. Perhaps a blog  Content Ideas  irreducible error is reducible (by adding predictors) the inappropriate use of the words \u0026ldquo;can/can\u0026rsquo;t\u0026rdquo; and \u0026ldquo;need\u0026rdquo; in statistics.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4de48ece2395eafcedb4b1ee38be6764","permalink":"/probabilistic-thinking/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/probabilistic-thinking/readme/","section":"probabilistic-thinking","summary":"\u0026ldquo;Probabilistic Thinking in Data Science\u0026rdquo; Initiative Resources to migrate to this new resource.\nVision for how people will engage:\n As if they are learning something for a course. Perhaps a blog  Content Ideas  irreducible error is reducible (by adding predictors) the inappropriate use of the words \u0026ldquo;can/can\u0026rsquo;t\u0026rdquo; and \u0026ldquo;need\u0026rdquo; in statistics.  ","tags":null,"title":"","type":"probabilistic-thinking"},{"authors":null,"categories":null,"content":"Our Curriculum, Part II This post is a continuation of our post from last summer on the role of computer science in the Master of Data Science (MDS) curriculum here at UBC-Vancouver. This time, we\u0026rsquo;re focussing on the role of Statistics in the program.\nWe see two main challenges in teaching Statistics in our program:\n Turning established statistical framework into a lens for data science. Fitting this into a 10-month program amongst other data science topics.  1. Statistics does not directly translate to data science Statistical science: to us, is the science of numerically describing uncertainty. But the things one talks about when discussing statistics depends on the end in mind.\nTypically and traditionally, the end involves establishing properties of a statistical method. For example, finding the asymptotic distributions of GLM estimates, or describing the usefulness of Pearson\u0026rsquo;s correlation in measuring dependence. These are method-first approaches, where we begin with a model (such as a GLM or two random variables with dependence), and explore its properties. A course in statistics that\u0026rsquo;s grouped by method tends to be indicative of a method-first approach: a lecture on the GLM starts by defining the GLM, then moves on to its properties and a description of how to implement it; a lecture on maximum likelihood estimation defines the likelihood function, the loss function, and the estimator\u0026rsquo;s asymptotic distribution.\nThe end for statistics for data science is a solution to a real-world problem, making this a problem-first approach to statistics. The method-first approach does not easily translate here. For example, how do we know when to use a GLM for a given problem? Sure, the method-first approach tells us what the data should look like before using a GLM, but the converse is not necessarily true: having data that looks like it fits a GLM does not mean that you should fit a GLM. A random forest might be better for your problem. Or, maybe it\u0026rsquo;s better to drop the Poisson assumption (yep, you can do that).\n2.\\ Statistics for Data Science To make a course in statistics with a problem-first approach, lectures should be grouped by (you guessed it) problem. As an example, our Regression II course has topics such as:\n Regression when the range of your response is restricted Regression when your data are censored Regression when some data are missing  In each scenario, we look at the pros and cons of approaches for two encompassing problem types: optimizing prediction, and gaining interpretation. We focus on pros and cons of making assumptions in two areas: the model function and the conditional distributions. For example, when the range of the response is restricted to being positive, we look at the value of a linear predictor with both the identity and log link functions, and the value of adding a Poisson assumption to the conditional distributions, to arrive at a GLM as one possible solution.\nIn our experience, statistics discussed from a problem-first approach is hard to come by, so we see this as an area deserving active research. Even applied statistics tends to take a method-first approach, the difference being a stronger focus on how to draw conclusions from a method on various problems. Or, sometimes applied statistics does start with a real problem, but then the bulk of the attention is spent developing a method, then describing its properties.\nHow to make a problem-first course We\u0026rsquo;ve found the following to be useful:\n Begin with a statistical method. Distill this down to a problem type by asking why one would want to use this method. Ask: What are the assumptions of the approach? Are they really \u0026ldquo;needed\u0026rdquo;?  Trimming To fit the concepts into a 10-month program, we use the principle that the tidyverse adopts, and that\u0026rsquo;s to establish a simple foundation that covers \u0026ldquo;most\u0026rdquo; situations. Sometimes, this means not discussing how a model is estimated (such as the proportional hazards model), but perhaps just giving a brief general idea in words.\nStat is still important before problem-first approach.\n Authors: Vincenzo Coia is a lecturer with the MDS Vancouver program in the UBC Department of Statistics.\nAcknowlegdements ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"247a80309218cc8a15779ee66f7f3e00","permalink":"/probabilistic-thinking/statistics-blog_post/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/probabilistic-thinking/statistics-blog_post/","section":"probabilistic-thinking","summary":"Our Curriculum, Part II This post is a continuation of our post from last summer on the role of computer science in the Master of Data Science (MDS) curriculum here at UBC-Vancouver. This time, we\u0026rsquo;re focussing on the role of Statistics in the program.\nWe see two main challenges in teaching Statistics in our program:\n Turning established statistical framework into a lens for data science. Fitting this into a 10-month program amongst other data science topics.","tags":null,"title":"","type":"probabilistic-thinking"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"57bf93109dcec65252926da1c8d0d700","permalink":"/project/coperate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/coperate/","section":"project","summary":"Draw powerful multivariate insights using copulas. R package. **Coming soon!**","tags":["R"],"title":"coperate","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ea2583cbcde5de3401ae36896d04eef5","permalink":"/project/distplyr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/distplyr/","section":"project","summary":"Draw powerful insights using distributions. R package.","tags":["R"],"title":"distplyr","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"49f206e15c8f8776b088cdb872e95b94","permalink":"/project/igcop/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/igcop/","section":"project","summary":"Tools for accessing the IG and IGL family of copulas. R package.","tags":["R"],"title":"igcop","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5aae1ec86a01c71db8ff29652d4f8282","permalink":"/project/interpreting-regression/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/interpreting-regression/","section":"project","summary":"An in-progress book on data-driven problem solving from a probabilistic lens. A modern take to statistics. Title may change.","tags":["Statistics"],"title":"Interpreting Regression","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6483462f3258f30aa0b56689dbdc6fa0","permalink":"/project/rqdist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/rqdist/","section":"project","summary":"Build predictive distributions using linear quantile regression. R package.","tags":["R"],"title":"rqdist","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a7aea7ec29e633b4e5f072e6d1b4ddad","permalink":"/project/stat545/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/stat545/","section":"project","summary":"An open-access course on data wrangling, exploration, and analysis using the modern R landscape with git.","tags":["STAT 545","R"],"title":"STAT 545 @ UBC","type":"project"},{"authors":null,"categories":null,"content":"  A major characteristic of data science is its focus on doing. My lessons take the form of autograded worksheets, which are given to the students in advance. The autograder allows students to instantaneously test the validity of a variety of solutions on their own. Live coding the worksheets in class allows students to compare their solutions to mine, and allows students to ask about offshoots to the questions. For an example, see the STAT 545 worksheets and the STAT 201 worksheets.\nAnother major characteristic of data science is its problem-first perspective (as opposed to a model-first perspective). Whenever possible, I draw inspiration from examples encountered in everyday life along with business needs inspired from my consulting and MDS capstone experience. For example, predicting the total number of wedding RSVPs motivates simulation, and preparing for floods motivates quantile regression. A problem-first focus identifies areas that are not well developed or not often taught in traditional statistics curricula, such as non-Gaussian predictive distributions. Filling these holes form the core of my educational leadership activities.\nGiving a course an open access platform through a website allows a wider reach and brings attention to our department and university. Also, open access is like an accountability partner: it provides incentive for producing higher quality resources. Plus, open access frees students from the need to squirrel away resources on their own. Open access gives the course an entity of its own; a community with a clear vision in which to join. For an example, see the STAT 545 course website: https://stat545.stat.ubc.ca/\nI give my teaching assistants the opportunity to add an additional line to their CVs by inviting them to deliver guest lectures and to develop course material. Together with bringing TA‚Äôs into the decision making process about course delivery and direction, the result is an engaged teaching team and an improved sense of learning and community for the students.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"52f5d279ebec4eec0a02f2bb4b543335","permalink":"/approach/teaching/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/approach/teaching/","section":"approach","summary":"A major characteristic of data science is its focus on doing. My lessons take the form of autograded worksheets, which are given to the students in advance. The autograder allows students to instantaneously test the validity of a variety of solutions on their own. Live coding the worksheets in class allows students to compare their solutions to mine, and allows students to ask about offshoots to the questions. For an example, see the STAT 545 worksheets and the STAT 201 worksheets.","tags":null,"title":"Teaching Philosophy","type":"approach"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"478d41a08bd0ce0385907859e59da585","permalink":"/project/youtube/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/youtube/","section":"project","summary":"Short videos promoting a clean and modern data analysis. Pairs with STAT 545.","tags":["STAT 545","R"],"title":"YouTube Channel","type":"project"}]